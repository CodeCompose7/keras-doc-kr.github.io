---
layout: default
title: ì½”ë“œ ì˜ˆì œ
nav_order: 6
permalink: /examples/
has_children: true
---

* ì›ë³¸ ë§í¬ : [https://keras.io/examples/](https://keras.io/examples/){:target="_blank"}
* ìµœì¢… ìˆ˜ì •ì¼ : 2024-04-22

# ì½”ë“œ ì˜ˆì œ
{: .no_toc }

## ëª©ì°¨
{: .no_toc .text-delta }

1. TOC
{:toc}

---

ì½”ë“œ ì˜ˆì œ (Code examples)
=============

ìš°ë¦¬ì˜ ì½”ë“œ ì˜ˆì œëŠ” 300ì¤„ ë¯¸ë§Œì˜ ì§§ì€ ì½”ë“œì´ë©°, ìˆ˜ì§ì  ë”¥ëŸ¬ë‹ ì›Œí¬í”Œë¡œìš°ì— ëŒ€í•œ ì§‘ì¤‘ì ì¸ ë°ëª¨ì…ë‹ˆë‹¤.

ëª¨ë“  ì˜ˆì œëŠ” Jupyter ë…¸íŠ¸ë¶ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë©°, ë³„ë„ì˜ ì„¤ì •ì´ í•„ìš” ì—†ê³  í´ë¼ìš°ë“œì—ì„œ ì‹¤í–‰ë˜ëŠ” í˜¸ìŠ¤íŒ… ë…¸íŠ¸ë¶ í™˜ê²½ì¸ [Google Colab](https://colab.research.google.com/notebooks/welcome.ipynb)ì—ì„œ í´ë¦­ í•œ ë²ˆìœ¼ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Google Colabì—ëŠ” GPU ë° TPU ëŸ°íƒ€ì„ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

â˜…
{: .label .label-purple .mx-1}
ì‹œì‘í•˜ê¸° ì¢‹ì€ ì˜ˆì œ

V3
{: .label .label-green .mx-1}
Keras 3 ì˜ˆì œ

[ì»´í“¨í„° ë¹„ì „]({% link docs/06-examples/01-vision.md %})
------------------------------------

### ì´ë¯¸ì§€ ë¶„ë¥˜

â˜…
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[ì²˜ìŒë¶€í„° ì´ë¯¸ì§€ ë¶„ë¥˜ (Image classification from scratch)]({% link docs/06-examples/01-vision/01-image_classification_from_scratch.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

â˜…
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[ê°„ë‹¨í•œ MNIST convnet (Simple MNIST convnet)]({% link docs/06-examples/01-vision/02-mnist_convnet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

â˜…
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[EfficientNetìœ¼ë¡œ í•˜ëŠ” ë¯¸ì„¸ ì¡°ì •ì„ í†µí•œ ì´ë¯¸ì§€ ë¶„ë¥˜ (Image classification via fine-tuning with EfficientNet)]({% link docs/06-examples/01-vision/03-image_classification_efficientnet_fine_tuning.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ ì´ë¯¸ì§€ ë¶„ë¥˜ (Image classification with Vision Transformer)]({% link docs/06-examples/01-vision/04-image_classification_with_vision_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì–´í…ì…˜ ê¸°ë°˜ ì‹¬ì¸µ ë‹¤ì¤‘ ì¸ìŠ¤í„´ìŠ¤ í•™ìŠµ(MIL)ì„ ì‚¬ìš©í•œ ë¶„ë¥˜ (Classification using Attention-based Deep Multiple Instance Learning)]({% link docs/06-examples/01-vision/05-attention_mil_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ìµœì‹  MLP ëª¨ë¸ì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ (Image classification with modern MLP models)]({% link docs/06-examples/01-vision/06-mlp_image_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìœ„í•œ ëª¨ë°”ì¼ ì¹œí™”ì ì¸ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ (A mobile-friendly Transformer-based model for image classification)]({% link docs/06-examples/01-vision/07-mobilevit.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[TPUì—ì„œ íë ´ ë¶„ë¥˜ (Pneumonia Classification on TPU)]({% link docs/06-examples/01-vision/08-xray_classification_with_tpus.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì»´íŒ©íŠ¸ ì»¨ë³¼ë£¨ì…˜ íŠ¸ëœìŠ¤í¬ë¨¸ (Compact Convolutional Transformers)]({% link docs/06-examples/01-vision/09-cct.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ConvMixerë¡œ ì´ë¯¸ì§€ ë¶„ë¥˜ (Image classification with ConvMixer)]({% link docs/06-examples/01-vision/10-convmixer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[EANet(External Attention Transformer)ì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ (Image classification with EANet (External Attention Transformer))]({% link docs/06-examples/01-vision/11-eanet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì¸ë³¼ë£¨ì…˜ ì‹ ê²½ë§ (Involutional neural networks)]({% link docs/06-examples/01-vision/12-involution.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Perceiverë¡œ ì´ë¯¸ì§€ ë¶„ë¥˜ (Image classification with Perceiver)]({% link docs/06-examples/01-vision/13-perceiver_image_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Reptileì„ ì‚¬ìš©í•œ í“¨ìƒ· í•™ìŠµ (Few-Shot learning with Reptile)]({% link docs/06-examples/01-vision/14-reptile.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[SimCLRì„ ì‚¬ìš©í•œ ëŒ€ì¡° ì‚¬ì „ íŠ¸ë ˆì´ë‹ì„ ì‚¬ìš©í•œ ë°˜ì§€ë„ ì´ë¯¸ì§€ ë¶„ë¥˜ (Semi-supervised image classification using contrastive pretraining with SimCLR)]({% link docs/06-examples/01-vision/15-semisupervised_simclr.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Swin íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ (Image classification with Swin Transformers)]({% link docs/06-examples/01-vision/16-swin_transformers.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ì†Œê·œëª¨ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•´ ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ íŠ¸ë ˆì´ë‹ (Train a Vision Transformer on small datasets)]({% link docs/06-examples/01-vision/17-vit_small_ds.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ì–´í…ì…˜ì´ ì—†ëŠ” ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ (A Vision Transformer without Attention)]({% link docs/06-examples/01-vision/18-shiftvit.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ê¸€ë¡œë²Œ ì»¨í…ìŠ¤íŠ¸ ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì´ìš©í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ (Image Classification using Global Context Vision Transformer)]({% link docs/06-examples/01-vision/19-image_classification_using_global_context_vision_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[BigTransfer(BiT)ë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ (Image Classification using BigTransfer (BiT))]({% link docs/06-examples/01-vision/20-bit.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜

â˜…
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[U-Netê³¼ ìœ ì‚¬í•œ ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ (Image segmentation with a U-Net-like architecture)]({% link docs/06-examples/01-vision/21-oxford_pets_image_segmentation.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[DeepLabV3+ë¥¼ ì‚¬ìš©í•œ ë‹¤ì¤‘ í´ë˜ìŠ¤ ì‹œë§¨í‹± ì„¸ê·¸ë©˜í…Œì´ì…˜ (Multiclass semantic segmentation using DeepLabV3+)]({% link docs/06-examples/01-vision/22-deeplabv3_plus.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[BASNetì„ ì‚¬ìš©í•œ ë§¤ìš° ì •í™•í•œ ê²½ê³„ ì„¸ê·¸ë©˜í…Œì´ì…˜ (Highly accurate boundaries segmentation using BASNet)]({% link docs/06-examples/01-vision/23-basnet_segmentation.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Composable ì™„ì „ ì»¨ë³¼ë£¨ì…˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ (Image Segmentation using Composable Fully-Convolutional Networks)]({% link docs/06-examples/01-vision/24-fully_convolutional_network.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ê°ì²´ ê°ì§€

V2
{: .label .label-yellow .mx-1}
[RetinaNetì„ ì´ìš©í•œ ê°ì²´ ê°ì§€ (Object Detection with RetinaNet)]({% link docs/06-examples/01-vision/25-retinanet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì „ì´ í•™ìŠµì„ í†µí•œ í‚¤í¬ì¸íŠ¸ ê°ì§€ (Keypoint Detection with Transfer Learning)]({% link docs/06-examples/01-vision/26-keypoint_detection.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•œ ê°ì²´ ê°ì§€ (Object detection with Vision Transformers)]({% link docs/06-examples/01-vision/27-object_detection_using_vision_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 3D

V3
{: .label .label-green .mx-1}
[CT ìŠ¤ìº”ì˜ 3D ì´ë¯¸ì§€ ë¶„ë¥˜ (3D image classification from CT scans)]({% link docs/06-examples/01-vision/28-3D_image_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ë‹¨ì•ˆ ê¹Šì´ ì¶”ì • (Monocular depth estimation)]({% link docs/06-examples/01-vision/29-depth_estimation.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

â˜…
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[NeRFë¥¼ ì‚¬ìš©í•œ 3D ì²´ì  ë Œë”ë§ (3D volumetric rendering with NeRF)]({% link docs/06-examples/01-vision/30-nerf.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[PointNetì„ ì‚¬ìš©í•œ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ì„¸ê·¸ë©˜í…Œì´ì…˜ (Point cloud segmentation with PointNet)]({% link docs/06-examples/01-vision/31-pointnet_segmentation.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[PointNetì„ ì‚¬ìš©í•œ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë¶„ë¥˜ (Point cloud classification)]({% link docs/06-examples/01-vision/32-pointnet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### OCR

V3
{: .label .label-green .mx-1}
[ìº¡ì±  ì½ê¸°ë¥¼ ìœ„í•œ OCR ëª¨ë¸ (OCR model for reading Captchas)]({% link docs/06-examples/01-vision/33-captcha_ocr.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ì†ê¸€ì”¨ ì¸ì‹ (Handwriting recognition)]({% link docs/06-examples/01-vision/34-handwriting_recognition.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ì´ë¯¸ì§€ ê°•í™”

V3
{: .label .label-green .mx-1}
[ì´ë¯¸ì§€ ë…¸ì´ì¦ˆ ì œê±°ë¥¼ ìœ„í•œ ì»¨ë³¼ë£¨ì…”ë„ ì˜¤í† ì¸ì½”ë” (Convolutional autoencoder for image denoising)]({% link docs/06-examples/01-vision/35-autoencoder.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[MIRNetì„ ì‚¬ìš©í•œ ì €ì¡°ë„ ì´ë¯¸ì§€ í–¥ìƒ (Low-light image enhancement using MIRNet)]({% link docs/06-examples/01-vision/36-mirnet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Efficient Sub-Pixel CNNì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ì´ˆí•´ìƒë„ (Image Super-Resolution using an Efficient Sub-Pixel CNN)]({% link docs/06-examples/01-vision/37-super_resolution_sub_pixel.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ë‹¨ì¼ ì´ë¯¸ì§€ ì´ˆí•´ìƒë„ë¥¼ ìœ„í•œ í–¥ìƒëœ ê¹Šì€ Residual ë„¤íŠ¸ì›Œí¬ (Enhanced Deep Residual Networks for single-image super-resolution)]({% link docs/06-examples/01-vision/38-edsr.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì €ì¡°ë„ ì´ë¯¸ì§€ í–¥ìƒì„ ìœ„í•œ Zero-DCE (Zero-DCE for low-light image enhancement)]({% link docs/06-examples/01-vision/39-zero_dce.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ë°ì´í„° ë³´ê°•

V3
{: .label .label-green .mx-1}
[ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìœ„í•œ CutMix ë°ì´í„° ë³´ê°• (CutMix data augmentation for image classification)]({% link docs/06-examples/01-vision/40-cutmix.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìœ„í•œ MixUp ë³´ê°• (MixUp augmentation for image classification)]({% link docs/06-examples/01-vision/41-mixup.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ê²¬ê³ ì„± í–¥ìƒì„ ìœ„í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìœ„í•œ RandAugment (RandAugment for Image Classification for Improved Robustness)]({% link docs/06-examples/01-vision/42-randaugment.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ì´ë¯¸ì§€ & í…ìŠ¤íŠ¸

â˜…
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[ì´ë¯¸ì§€ ìº¡ì…˜ (Image captioning)]({% link docs/06-examples/01-vision/43-image_captioning.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ë“€ì–¼ ì¸ì½”ë”ë¥¼ ì´ìš©í•œ ìì—°ì–´ ì´ë¯¸ì§€ ê²€ìƒ‰ (Natural language image search with a Dual Encoder)]({% link docs/06-examples/01-vision/44-nl_image_search.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ë¹„ì „ ëª¨ë¸ í•´ì„ ê°€ëŠ¥ì„±(interpretability)

V3
{: .label .label-green .mx-1}
[Convnetsì´ í•™ìŠµí•œ ë‚´ìš© ì‹œê°í™” (Visualizing what convnets learn)]({% link docs/06-examples/01-vision/45-visualizing_what_convnets_learn.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[í†µí•© ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ í†µí•œ ëª¨ë¸ í•´ì„ ê°€ëŠ¥ì„± (Model interpretability with Integrated Gradients)]({% link docs/06-examples/01-vision/46-integrated_gradients.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ í‘œí˜„ ì¡°ì‚¬ (Investigating Vision Transformer representations)]({% link docs/06-examples/01-vision/47-probing_vits.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Grad-CAM í´ë˜ìŠ¤ í™œì„±í™” ì‹œê°í™” (Grad-CAM class activation visualization)]({% link docs/06-examples/01-vision/48-grad_cam.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê²€ìƒ‰

V2
{: .label .label-yellow .mx-1}
[ì¤‘ë³µì— ê°€ê¹Œìš´ ì´ë¯¸ì§€ ê²€ìƒ‰ (Near-duplicate image search)]({% link docs/06-examples/01-vision/49-near_dup_search.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì‹œë§¨í‹± ì´ë¯¸ì§€ í´ëŸ¬ìŠ¤í„°ë§ (Semantic Image Clustering)]({% link docs/06-examples/01-vision/50-semantic_image_clustering.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ëŒ€ë¹„ ì†ì‹¤ì´ ìˆëŠ” Siamese ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ìœ ì‚¬ë„ ì¶”ì • (Image similarity estimation using a Siamese Network with a contrastive loss)]({% link docs/06-examples/01-vision/51-siamese_contrastive.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì‚¼ì¤‘(triplet) ì†ì‹¤ì´ ìˆëŠ” Siamese ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ìœ ì‚¬ë„ ì¶”ì • (Image similarity estimation using a Siamese Network with a triplet loss)]({% link docs/06-examples/01-vision/52-siamese_network.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìœ„í•œ ë©”íŠ¸ë¦­ í•™ìŠµ (Metric learning for image similarity search)]({% link docs/06-examples/01-vision/53-metric_learning.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[TensorFlow Similarityë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìœ„í•œ ë©”íŠ¸ë¦­ í•™ìŠµ (Metric learning for image similarity search using TensorFlow Similarity)]({% link docs/06-examples/01-vision/54-metric_learning_tf_similarity.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[NNCLRì„ ì‚¬ìš©í•œ ìê¸° ì§€ë„ ëŒ€ì¡° í•™ìŠµ (Self-supervised contrastive learning with NNCLR)]({% link docs/06-examples/01-vision/55-nnclr.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ë¹„ë””ì˜¤

V3
{: .label .label-green .mx-1}
[CNN-RNN ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•œ ë¹„ë””ì˜¤ ë¶„ë¥˜ (Video Classification with a CNN-RNN Architecture)]({% link docs/06-examples/01-vision/56-video_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì»¨ë³¼ë£¨ì…˜ LSTMì„ ì‚¬ìš©í•œ ë‹¤ìŒ í”„ë ˆì„ ë¹„ë””ì˜¤ ì˜ˆì¸¡ (Next-Frame Video Prediction with Convolutional LSTMs)]({% link docs/06-examples/01-vision/57-conv_lstm.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•œ ë¹„ë””ì˜¤ ë¶„ë¥˜ (Video Classification with Transformers)]({% link docs/06-examples/01-vision/58-video_transformers.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ë¹„ë””ì˜¤ ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ (Video Vision Transformer)]({% link docs/06-examples/01-vision/59-vivit.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ì„±ëŠ¥ ë ˆì‹œí”¼

V3
{: .label .label-green .mx-1}
[íŠ¸ë ˆì´ë‹ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ê·¸ë˜ë””ì–¸íŠ¸ ì¤‘ì•™í™” (Gradient Centralization for Better Training Performance)]({% link docs/06-examples/01-vision/60-gradient_centralization.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ í† í°í™” í•™ìŠµí•˜ê¸° (Learning to tokenize in Vision Transformers)]({% link docs/06-examples/01-vision/61-token_learner.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì§€ì‹ ì¦ë¥˜ (Knowledge Distillation)]({% link docs/06-examples/01-vision/62-knowledge_distillation.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[FixRes: íŠ¸ë ˆì´ë‹-í…ŒìŠ¤íŠ¸ í•´ìƒë„ ë¶ˆì¼ì¹˜ ìˆ˜ì • (FixRes: Fixing train-test resolution discrepancy)]({% link docs/06-examples/01-vision/63-fixres.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[LayerScaleì„ ì‚¬ìš©í•œ í´ë˜ìŠ¤ ì–´í…ì…˜ ì´ë¯¸ì§€ íŠ¸ëœìŠ¤í¬ë¨¸ (Class Attention Image Transformers with LayerScale)]({% link docs/06-examples/01-vision/64-cait.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[í†µí•© ì–´í…ì…˜ìœ¼ë¡œ ì»¨ë³¼ë£¨ì…˜ ë„¤íŠ¸ì›Œí¬ ê°•í™” (Augmenting convnets with aggregated attention)]({% link docs/06-examples/01-vision/65-patch_convnet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì»´í“¨í„° ë¹„ì „ì—ì„œ ë¦¬ì‚¬ì´ì¦ˆ í•™ìŠµ (Learning to Resize)]({% link docs/06-examples/01-vision/66-learnable_resizer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ê¸°íƒ€

V2
{: .label .label-yellow .mx-1}
[AdaMatchë¥¼ í†µí•œ ë°˜ì§€ë„ ë° ë„ë©”ì¸ ì ì‘ (Semi-supervision and domain adaptation with AdaMatch)]({% link docs/06-examples/01-vision/67-adamatch.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Contrastive SSLì„ ìœ„í•œ Barlow Twins (Barlow Twins for Contrastive SSL)]({% link docs/06-examples/01-vision/68-barlow_twins.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ì§€ë„ë¥¼ í†µí•œ ì¼ê´€ì„± íŠ¸ë ˆì´ë‹ (Consistency training with supervision)]({% link docs/06-examples/01-vision/69-consistency_training.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ì¦ë¥˜ì‹ ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ (Distilling Vision Transformers)]({% link docs/06-examples/01-vision/70-deit.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ì´ˆì  ë³€ì¡°(Focal Modulation): ì…€í”„ ì–´í…ì…˜ì„ ëŒ€ì²´í•˜ëŠ” (Focal Modulation: A replacement for Self-Attention)]({% link docs/06-examples/01-vision/71-focal_modulation_network.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìœ„í•œ Forward-Forward ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš© (Using the Forward-Forward Algorithm for Image Classification)]({% link docs/06-examples/01-vision/72-forwardforward.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ìë™ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•œ ë§ˆìŠ¤í¬ ì´ë¯¸ì§€ ëª¨ë¸ë§ (Masked image modeling with Autoencoders)]({% link docs/06-examples/01-vision/73-masked_image_modeling.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ğŸ¤— íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ ë¬´ì—‡ì´ë“  ëª¨ë¸ ì„¸ê·¸ë¨¼íŠ¸ (Segment Anything Model with ğŸ¤—Transformers)]({% link docs/06-examples/01-vision/74-sam.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[SegFormerì™€ Hugging Face íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•œ ì‹œë§¨í‹± ì„¸ê·¸ë©˜í…Œì´ì…˜ (Semantic segmentation with SegFormer and Hugging Face Transformers)]({% link docs/06-examples/01-vision/75-segformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[SimSiamì„ ì´ìš©í•œ ìê¸° ì§€ë„ ëŒ€ì¡° í•™ìŠµ (Self-supervised contrastive learning with SimSiam)]({% link docs/06-examples/01-vision/76-simsiam.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ì§€ë„ ëŒ€ì¡° í•™ìŠµ (Supervised Contrastive Learning)]({% link docs/06-examples/01-vision/77-supervised-contrastive-learning.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Recurrenceì™€ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ë§Œë‚¨ (When Recurrence meets Transformers)]({% link docs/06-examples/01-vision/78-temporal_latent_bottleneck.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[YOLOV8 ë° KerasCVë¥¼ í†µí•œ íš¨ìœ¨ì ì¸ ê°ì²´ ê°ì§€ (Efficient Object Detection with YOLOV8 and KerasCV)]({% link docs/06-examples/01-vision/79-yolov8.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

* * *

[ìì—°ì–´ ì²˜ë¦¬ (Natural Language Processing)]({% link docs/06-examples/02-nlp.md %})
---------------------------------------------

### í…ìŠ¤íŠ¸ ë¶„ë¥˜

â˜…
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[ì²˜ìŒë¶€í„° í…ìŠ¤íŠ¸ ë¶„ë¥˜ (Text classification from scratch)]({% link docs/06-examples/02-nlp/01-text_classification_from_scratch.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Active í•™ìŠµì„ ì‚¬ìš©í•œ ë¶„ë¥˜ ë¦¬ë·° (Review Classification using Active Learning)]({% link docs/06-examples/02-nlp/02-active_learning_review_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[FNetì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ (Text Classification using FNet)]({% link docs/06-examples/02-nlp/03-fnet_classification_with_keras_nlp.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ëŒ€ê·œëª¨ ë‹¤ì¤‘ ë ˆì´ë¸” í…ìŠ¤íŠ¸ ë¶„ë¥˜ (Large-scale multi-label text classification)]({% link docs/06-examples/02-nlp/04-multi_label_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ (Text classification with Transformer)]({% link docs/06-examples/02-nlp/05-text_classification_with_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ìŠ¤ìœ„ì¹˜ íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ (Text classification with Switch Transformer)]({% link docs/06-examples/02-nlp/06-text_classification_with_switch_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ì˜ì‚¬ ê²°ì • í¬ë ˆìŠ¤íŠ¸ì™€ ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ ì„ë² ë”©ì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ (Text classification using Decision Forests and pretrained embeddings)]({% link docs/06-examples/02-nlp/07-tweet-classification-using-tfdf.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ ë‹¨ì–´ ì„ë² ë”© ì‚¬ìš© (Using pre-trained word embeddings)]({% link docs/06-examples/02-nlp/08-pretrained_word_embeddings.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[IMDBì— ëŒ€í•œ ì–‘ë°©í–¥ LSTM (Bidirectional LSTM on IMDB)]({% link docs/06-examples/02-nlp/09-bidirectional_lstm_imdb.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[KerasNLP ë° tf.distributeë¥¼ ì‚¬ìš©í•œ ë°ì´í„° ë³‘ë ¬ íŠ¸ë ˆì´ë‹ (Data Parallel Training with KerasNLP and tf.distribute)]({% link docs/06-examples/02-nlp/10-data_parallel_training_with_keras_nlp.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ê¸°ê³„ ë²ˆì—­

V3
{: .label .label-green .mx-1}
[KerasNLPë¥¼ ì‚¬ìš©í•œ ì˜ì–´-ìŠ¤í˜ì¸ì–´ ë²ˆì—­ (English-to-Spanish translation with KerasNLP)]({% link docs/06-examples/02-nlp/11-neural_machine_translation_with_keras_nlp.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

â˜…
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[ì‹œí€€ìŠ¤-to-ì‹œí€€ìŠ¤ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•œ ì˜ì–´-ìŠ¤í˜ì¸ì–´ ë²ˆì—­ (English-to-Spanish translation with a sequence-to-sequence Transformer)]({% link docs/06-examples/02-nlp/12-neural_machine_translation_with_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ë¬¸ì ë ˆë²¨ recurrent ì‹œí€€ìŠ¤-to-ì‹œí€€ìŠ¤ ëª¨ë¸ (Character-level recurrent sequence-to-sequence model)]({% link docs/06-examples/02-nlp/13-lstm_seq2seq.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ì—°ê´€ì„± ì˜ˆì¸¡ (Entailment prediction)

V2
{: .label .label-yellow .mx-1}
[ë©€í‹°ëª¨ë‹¬ ìˆ˜ë°˜ (Multimodal entailment)]({% link docs/06-examples/02-nlp/14-multimodal_entailment.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ëª…ëª…ëœ ì—”í‹°í‹° ì¸ì‹

V3
{: .label .label-green .mx-1}
[íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•œ ëª…ëª…ëœ ì—”í‹°í‹° ì¸ì‹ (Named Entity Recognition using Transformers)]({% link docs/06-examples/02-nlp/15-ner_transformers.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Sequence-to-sequence

V2
{: .label .label-yellow .mx-1}
[BERTë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (Text Extraction with BERT)]({% link docs/06-examples/02-nlp/16-text_extraction_with_bert.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ìˆ«ì ë§ì…ˆì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ì‹œí€€ìŠ¤-to-ì‹œí€€ìŠ¤ í•™ìŠµ (Sequence to sequence learning for performing number addition)]({% link docs/06-examples/02-nlp/17-addition_rnn.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê²€ìƒ‰

V3
{: .label .label-green .mx-1}
[KerasNLPë¥¼ ì‚¬ìš©í•œ ì‹œë§¨í‹± ìœ ì‚¬ì„± (Semantic Similarity with KerasNLP)]({% link docs/06-examples/02-nlp/18-semantic_similarity_with_keras_nlp.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[BERTë¥¼ ì‚¬ìš©í•œ ì‹œë§¨í‹± ìœ ì‚¬ì„± (Semantic Similarity with BERT)]({% link docs/06-examples/02-nlp/19-semantic_similarity_with_bert.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Siamese RoBERTa ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•œ ë¬¸ì¥ ì„ë² ë”© (Sentence embeddings using Siamese RoBERTa-networks)]({% link docs/06-examples/02-nlp/20-sentence_embeddings_with_sbert.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ì–¸ì–´ ëª¨ë¸ë§

V2
{: .label .label-yellow .mx-1}
[BERTë¥¼ ì‚¬ìš©í•œ ì—”ë“œíˆ¬ì—”ë“œ ë§ˆìŠ¤í¬ ì–¸ì–´ ëª¨ë¸ë§ (End-to-end Masked Language Modeling with BERT)]({% link docs/06-examples/02-nlp/21-masked_language_modeling.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Hugging Face íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ BERT ì‚¬ì „ íŠ¸ë ˆì´ë‹í•˜ê¸° (Pretraining BERT with Hugging Face Transformers)]({% link docs/06-examples/02-nlp/22-pretraining_BERT.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### íš¨ìœ¨ì ì¸ ë§¤ê°œë³€ìˆ˜ ë¯¸ì„¸ ì¡°ì •

V3
{: .label .label-green .mx-1}
[LoRAê°€ ìˆëŠ” GPT-2ì˜ íš¨ìœ¨ì ì¸ íŒŒë¼ë¯¸í„° ë¯¸ì„¸ ì¡°ì • (Parameter-efficient fine-tuning of GPT-2 with LoRA)]({% link docs/06-examples/02-nlp/23-parameter_efficient_finetuning_of_gpt2_with_lora.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ê¸°íƒ€

V2
{: .label .label-yellow .mx-1}
[BARTë¥¼ ì‚¬ìš©í•œ ì¶”ìƒì  í…ìŠ¤íŠ¸ ìš”ì•½ (Abstractive Text Summarization with BART)]({% link docs/06-examples/02-nlp/24-abstractive_summarization_with_bart.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ğŸ¤— íŠ¸ëœìŠ¤í¬ë¨¸ ë° TPUë¥¼ ì‚¬ìš©í•˜ì—¬ ì²˜ìŒë¶€í„° ì–¸ì–´ ëª¨ë¸ íŠ¸ë ˆì´ë‹í•˜ê¸° (Training a language model from scratch with ğŸ¤— Transformers and TPUs)]({% link docs/06-examples/02-nlp/25-mlm_training_tpus.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ì „ì´ í•™ìŠµìœ¼ë¡œ ê°ê´€ì‹ ê³¼ì œ (MultipleChoice Task with Transfer Learning)]({% link docs/06-examples/02-nlp/26-multiple_choice_task_with_transfer_learning.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Hugging Face íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ ì§ˆë¬¸ ë‹µë³€í•˜ê¸° (Question Answering with Hugging Face Transformers)]({% link docs/06-examples/02-nlp/27-question_answering.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Hugging Face íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•œ ì¶”ìƒì  ìš”ì•½ (Abstractive Summarization with Hugging Face Transformers)]({% link docs/06-examples/02-nlp/28-t5_hf_summarization.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

* * *

[êµ¬ì¡°í™”ëœ ë°ì´í„°]({% link docs/06-examples/03-structured_data.md %})
---------------------------------------------

### êµ¬ì¡°í™”ëœ ë°ì´í„° ë¶„ë¥˜

â˜…
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[FeatureSpaceë¥¼ ì‚¬ìš©í•œ êµ¬ì¡°í™”ëœ ë°ì´í„° ë¶„ë¥˜ (Structured data classification with FeatureSpace)]({% link docs/06-examples/03-structured_data/01-structured_data_classification_with_feature_space.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

â˜…
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[ë¶ˆê· í˜• ë¶„ë¥˜: ì‹ ìš© ì¹´ë“œ ì‚¬ê¸° íƒì§€ (Imbalanced classification: credit card fraud detection)]({% link docs/06-examples/03-structured_data/02-imbalanced_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì²˜ìŒë¶€í„° êµ¬ì¡°í™”ëœ ë°ì´í„° ë¶„ë¥˜ (Structured data classification from scratch)]({% link docs/06-examples/03-structured_data/03-structured_data_classification_from_scratch.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì™€ì´ë“œ, ë”¥, í¬ë¡œìŠ¤ ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•œ êµ¬ì¡°í™”ëœ ë°ì´í„° í•™ìŠµ (Structured data learning with Wide, Deep, and Cross networks)]({% link docs/06-examples/03-structured_data/04-wide_deep_cross_networks.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Gated Residual ë° ë³€ìˆ˜ ì„ íƒ ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•œ ë¶„ë¥˜ (Classification with Gated Residual and Variable Selection Networks)]({% link docs/06-examples/03-structured_data/05-classification_with_grn_and_vsn.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[TensorFlow ì˜ì‚¬ ê²°ì • í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•œ ë¶„ë¥˜ (Classification with TensorFlow Decision Forests)]({% link docs/06-examples/03-structured_data/06-classification_with_tfdf.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì‹ ê²½ ì˜ì‚¬ ê²°ì • í¬ë ˆìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•œ ë¶„ë¥˜ (Classification with Neural Decision Forests)]({% link docs/06-examples/03-structured_data/07-deep_neural_decision_forests.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[TabTransformerë¥¼ ì‚¬ìš©í•œ êµ¬ì¡°í™”ëœ ë°ì´í„° í•™ìŠµ (Structured data learning with TabTransformer)]({% link docs/06-examples/03-structured_data/08-tabtransformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ì¶”ì²œ

V3
{: .label .label-green .mx-1}
[ì˜í™” ì¶”ì²œì„ ìœ„í•œ Collaborative í•„í„°ë§ (Collaborative Filtering for Movie Recommendations)]({% link docs/06-examples/03-structured_data/09-collaborative_filtering_movielens.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ (A Transformer-based recommendation system)]({% link docs/06-examples/03-structured_data/10-movielens_recommendations_transformers.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ê¸°íƒ€

V2
{: .label .label-yellow .mx-1}
[FeatureSpace ê³ ê¸‰ ì‚¬ìš© ì‚¬ë¡€ (FeatureSpace advanced use cases)]({% link docs/06-examples/03-structured_data/11-feature_space_advanced.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

* * *

[íƒ€ì„ì‹œë¦¬ì¦ˆ]({% link docs/06-examples/04-timeseries.md %})
-----------------------------------

### íƒ€ì„ì‹œë¦¬ì¦ˆ ë¶„ë¥˜

â˜…
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[ì²˜ìŒë¶€í„° ì‹œê³„ì—´ ë¶„ë¥˜ (Timeseries classification from scratch)]({% link docs/06-examples/04-timeseries/01-timeseries_classification_from_scratch.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ì‚¬ìš©í•œ ì‹œê³„ì—´ ë¶„ë¥˜ (Timeseries classification with a Transformer model)]({% link docs/06-examples/04-timeseries/02-timeseries_classification_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[í–‰ë™ ì‹ë³„ì„ ìœ„í•œ ë‡ŒíŒŒ ì‹ í˜¸ ë¶„ë¥˜ (Electroencephalogram Signal Classification for action identification)]({% link docs/06-examples/04-timeseries/03-eeg_signal_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ê²°ì œ ì¹´ë“œ ì‚¬ê¸° íƒì§€ë¥¼ ìœ„í•œ ì´ë²¤íŠ¸ ë¶„ë¥˜ (Event classification for payment card fraud detection)]({% link docs/06-examples/04-timeseries/04-event_classification_for_payment_card_fraud_detection.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ì´ìƒ ì§•í›„ íƒì§€

V3
{: .label .label-green .mx-1}
[ìë™ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•œ ì‹œê³„ì—´ ì´ìƒ íƒì§€ (Timeseries anomaly detection using an Autoencoder)]({% link docs/06-examples/04-timeseries/05-timeseries_anomaly_detection.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### íƒ€ì„ì‹œë¦¬ì¦ˆ ì˜ˆì¸¡

V3
{: .label .label-green .mx-1}
[ê·¸ë˜í”„ ì‹ ê²½ë§ê³¼ LSTMì„ ì‚¬ìš©í•œ íŠ¸ë˜í”½ ì˜ˆì¸¡ (Traffic forecasting using graph neural networks and LSTM)]({% link docs/06-examples/04-timeseries/06-timeseries_traffic_forecasting.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ë‚ ì”¨ ì˜ˆì¸¡ì„ ìœ„í•œ ì‹œê³„ì—´ ì˜ˆì¸¡ (Timeseries forecasting for weather prediction)]({% link docs/06-examples/04-timeseries/07-timeseries_weather_forecasting.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

* * *

[ìƒì„±í˜• ë”¥ëŸ¬ë‹]({% link docs/06-examples/05-generative.md %})
-------------------------------------------------

### ì´ë¯¸ì§€ ìƒì„±

â˜…
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[ë…¸ì´ì¦ˆ ì œê±° í™•ì‚° ì•”ì‹œì  ëª¨ë¸ (Denoising Diffusion Implicit Models)]({% link docs/06-examples/05-generative/01-ddim.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

â˜…
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[ì•ˆì •ì ì¸ Diffusionìœ¼ë¡œ ì ì¬ëœ ê³µê°„ ê±·ê¸° (A walk through latent space with Stable Diffusion)]({% link docs/06-examples/05-generative/02-random_walks_with_stable_diffusion.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[DreamBooth]({% link docs/06-examples/05-generative/03-dreambooth.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ë…¸ì´ì¦ˆ ì œê±° í™•ì‚° í™•ë¥ ë¡ ì  ëª¨ë¸ (Denoising Diffusion Probabilistic Models)]({% link docs/06-examples/05-generative/04-ddpm.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[í…ìŠ¤íŠ¸ ë°˜ì „ì„ í†µí•´ StableDiffusionì˜ ìƒˆë¡œìš´ ê°œë… ê°€ë¥´ì¹˜ê¸° (Teach StableDiffusion new concepts via Textual Inversion)]({% link docs/06-examples/05-generative/05-fine_tune_via_textual_inversion.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ì•ˆì •ì ì¸ í™•ì‚° ë¯¸ì„¸ ì¡°ì • (Fine-tuning Stable Diffusion)]({% link docs/06-examples/05-generative/06-finetune_stable_diffusion.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ë³€í˜• ìë™ì¸ì½”ë” (Variational AutoEncoder)]({% link docs/06-examples/05-generative/07-vae.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Model.train\_stepì„ ì˜¤ë²„ë¼ì´ë”© í•˜ëŠ” GAN (GAN overriding Model.train\_step)]({% link docs/06-examples/05-generative/08-dcgan_overriding_train_step.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[`Model.train_step`ì„ ì˜¤ë²„ë¼ì´ë”© í•˜ëŠ” WGAN-GP (WGAN-GP overriding Model.train\_step)]({% link docs/06-examples/05-generative/09-wgan_gp.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì¡°ê±´ë¶€ GAN (Conditional GAN)]({% link docs/06-examples/05-generative/10-conditional_gan.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[CycleGAN]({% link docs/06-examples/05-generative/11-cyclegan.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ì ì‘í˜• íŒë³„ì ì¦ê°•ì„ í†µí•œ ë°ì´í„° íš¨ìœ¨ì  GAN (Data-efficient GANs with Adaptive Discriminator Augmentation)]({% link docs/06-examples/05-generative/12-gan_ada.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Deep Dream]({% link docs/06-examples/05-generative/13-deep_dream.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì¡°ê±´ë¶€ ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•œ GauGAN (GauGAN for conditional image generation)]({% link docs/06-examples/05-generative/14-gaugan.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[PixelCNN]({% link docs/06-examples/05-generative/15-pixelcnn.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[StyleGANìœ¼ë¡œ ì–¼êµ´ ì´ë¯¸ì§€ ìƒì„± (Face image generation with StyleGAN)]({% link docs/06-examples/05-generative/16-stylegan.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ë²¡í„°í™”ëœ ë³€í˜• ìë™ ì¸ì½”ë” (Vector-Quantized Variational Autoencoders)]({% link docs/06-examples/05-generative/17-vq_vae.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

### ìŠ¤íƒ€ì¼ ì „ì´

V3
{: .label .label-green .mx-1}
[ì‹ ê²½ ìŠ¤íƒ€ì¼ ì „ì†¡ (Neural style transfer)]({% link docs/06-examples/05-generative/18-neural_style_transfer.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[AdaINì„ ì‚¬ìš©í•œ ì‹ ê²½ ìŠ¤íƒ€ì¼ ì „ì†¡ (Neural Style Transfer with AdaIN)]({% link docs/06-examples/05-generative/19-adain.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

### í…ìŠ¤íŠ¸ ìƒì„±

â˜…
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[KerasNLPë¥¼ ì‚¬ìš©í•œ GPT2 í…ìŠ¤íŠ¸ ìƒì„± (GPT2 Text Generation with KerasNLP)]({% link docs/06-examples/05-generative/20-gpt2_text_generation_with_kerasnlp.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[KerasNLPë¡œ ì²˜ìŒë¶€í„° GPT í…ìŠ¤íŠ¸ ìƒì„±í•˜ê¸° (GPT text generation from scratch with KerasNLP)]({% link docs/06-examples/05-generative/21-text_generation_gpt.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ë¯¸ë‹ˆì–´ì²˜ GPTë¡œ í…ìŠ¤íŠ¸ ìƒì„± (Text generation with a miniature GPT)]({% link docs/06-examples/05-generative/22-text_generation_with_miniature_gpt.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[LSTMì„ ì‚¬ìš©í•œ ë¬¸ì ë ˆë²¨ í…ìŠ¤íŠ¸ ìƒì„± (Character-level text generation with LSTM)]({% link docs/06-examples/05-generative/23-lstm_character_level_text_generation.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[FNetì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ìƒì„± (Text Generation using FNet)]({% link docs/06-examples/05-generative/24-text_generation_fnet.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

### ê·¸ë˜í”„ ìƒì„±

V2
{: .label .label-yellow .mx-1}
[VAEë¥¼ ì‚¬ìš©í•œ ì•½ë¬¼ ë¶„ì ìƒì„± (Drug Molecule Generation with VAE)]({% link docs/06-examples/05-generative/25-molecule_generation.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ì‘ì€ ë¶„ì ê·¸ë˜í”„ ìƒì„±ì„ ìœ„í•œ R-GCNì´ í¬í•¨ëœ WGAN-GP (WGAN-GP with R-GCN for the generation of small molecular graphs)]({% link docs/06-examples/05-generative/26-wgan-graphs.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

### ê¸°íƒ€

V2
{: .label .label-yellow .mx-1}
[Real NVPë¥¼ ì‚¬ìš©í•œ ë°€ë„ ì¶”ì • (Density estimation using Real NVP)]({% link docs/06-examples/05-generative/27-real_nvp.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

* * *

[ì˜¤ë””ì˜¤ ë°ì´í„°]({% link docs/06-examples/06-audio.md %})
------------------------------

### ìŒì„± ì¸ì‹

V3
{: .label .label-green .mx-1}
[íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ í†µí•œ ìë™ ìŒì„± ì¸ì‹ (Automatic Speech Recognition with Transformer)]({% link docs/06-examples/06-audio/01-transformer_asr.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

### ê¸°íƒ€

V2
{: .label .label-yellow .mx-1}
[CTCë¥¼ ì‚¬ìš©í•œ ìë™ ìŒì„± ì¸ì‹ (Automatic Speech Recognition using CTC)]({% link docs/06-examples/06-audio/02-ctc_asr.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[íŠ¹ì§• ë§¤ì¹­ì„ ì‚¬ìš©í•œ MelGAN ê¸°ë°˜ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ë°˜ì „ (MelGAN-based spectrogram inversion using feature matching)]({% link docs/06-examples/06-audio/03-melgan_spectrogram_inversion.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[í™”ì ì¸ì‹ (Speaker Recognition)]({% link docs/06-examples/06-audio/04-speaker_recognition_using_cnn.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ì „ì´ í•™ìŠµì„ ì‚¬ìš©í•œ ì˜ì–´ í™”ì ì–µì–‘ ì¸ì‹ (English speaker accent recognition using Transfer Learning)]({% link docs/06-examples/06-audio/05-uk_ireland_accent_recognition.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Hugging Face íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•œ ì˜¤ë””ì˜¤ ë¶„ë¥˜ (Audio Classification with Hugging Face Transformers)]({% link docs/06-examples/06-audio/06-wav2vec2_audiocls.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

* * *

[Reinforcement Learning]({% link docs/06-examples/07-rl.md %})
---------------------------------------

### RL ì•Œê³ ë¦¬ì¦˜

V3
{: .label .label-green .mx-1}
[Actor Critic Method]({% link docs/06-examples/07-rl/01-actor_critic_cartpole.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Proximal Policy Optimization]({% link docs/06-examples/07-rl/02-ppo_cartpole.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Deep Q-Learning for Atari Breakout]({% link docs/06-examples/07-rl/03-deep_q_network_breakout.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

### ê¸°íƒ€

V2
{: .label .label-yellow .mx-1}
[Deep Deterministic Policy Gradient (DDPG)]({% link docs/06-examples/07-rl/04-ddpg_pendulum.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

* * *

[Graph Data]({% link docs/06-examples/08-graph.md %})
------------------------------

### ê·¸ë˜í”„ ë°ì´í„° (Graph Data)

V2
{: .label .label-yellow .mx-1}
[Graph attention network (GAT) for node classification]({% link docs/06-examples/08-graph/01-gat_node_classification.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Node Classification with Graph Neural Networks]({% link docs/06-examples/08-graph/02-gnn_citations.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Message-passing neural network (MPNN) for molecular property prediction]({% link docs/06-examples/08-graph/03-mpnn-molecular-graphs.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Graph representation learning with node2vec]({% link docs/06-examples/08-graph/04-node2vec_movielens.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

* * *

[Quick Keras Recipes]({% link docs/06-examples/09-keras_recipes.md %})
-----------------------------------------------

### ì„œë¹„ìŠ¤

V3
{: .label .label-green .mx-1}
[Serving TensorFlow models with TFServing]({% link docs/06-examples/09-keras_recipes/01-tf_serving.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

### Keras ì‚¬ìš© íŒ

V3
{: .label .label-green .mx-1}
[Keras debugging tips]({% link docs/06-examples/09-keras_recipes/02-debugging_tips.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Customizing the convolution operation of a Conv2D layer]({% link docs/06-examples/09-keras_recipes/03-subclassing_conv_layers.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Trainer pattern]({% link docs/06-examples/09-keras_recipes/04-trainer_pattern.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Endpoint layer pattern]({% link docs/06-examples/09-keras_recipes/05-endpoint_layer_pattern.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Reproducibility in Keras Models]({% link docs/06-examples/09-keras_recipes/06-reproducibility_recipes.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Writing Keras Models With TensorFlow NumPy]({% link docs/06-examples/09-keras_recipes/07-tensorflow_numpy_models.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Simple custom layer example: Antirectifier]({% link docs/06-examples/09-keras_recipes/08-antirectifier.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Packaging Keras models for wide distribution using Functional Subclassing]({% link docs/06-examples/09-keras_recipes/09-packaging_keras_models_for_wide_distribution.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

### ML ëª¨ë²” ì‚¬ë¡€

V3
{: .label .label-green .mx-1}
[Estimating required sample size for model training]({% link docs/06-examples/09-keras_recipes/10-sample_size_estimate.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Memory-efficient embeddings for recommendation systems]({% link docs/06-examples/09-keras_recipes/11-memory_efficient_embeddings.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Creating TFRecords]({% link docs/06-examples/09-keras_recipes/12-creating_tfrecords.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

### ê¸°íƒ€

V2
{: .label .label-yellow .mx-1}
[Approximating non-Function Mappings with Mixture Density Networks]({% link docs/06-examples/09-keras_recipes/13-approximating_non_function_mappings.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Probabilistic Bayesian Neural Networks]({% link docs/06-examples/09-keras_recipes/14-bayesian_neural_networks.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Knowledge distillation recipes]({% link docs/06-examples/09-keras_recipes/15-better_knowledge_distillation.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Evaluating and exporting scikit-learn metrics in a Keras callback]({% link docs/06-examples/09-keras_recipes/16-sklearn_metric_callbacks.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[How to train a Keras model on TFRecord files]({% link docs/06-examples/09-keras_recipes/17-tfrecord.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

* * *

Adding a new code example
-------------------------

We welcome new code examples! Here are our rules:

*   They should be shorter than 300 lines of code (comments may be as long as you want).
*   They should demonstrate modern Keras best practices.
*   They should be substantially different in topic from all examples listed above.
*   They should be extensively documented & commented.

New examples are added via Pull Requests to the [keras.io repository](https://github.com/keras-team/keras-io). They must be submitted as a `.py` file that follows a specific format. They are usually generated from Jupyter notebooks. See the [`tutobooks` documentation](https://github.com/keras-team/keras-io/blob/master/README.md) for more details.

If you would like to convert a Keras 2 example to Keras 3, please open a Pull Request to the [keras.io repository](https://github.com/keras-team/keras-io).
