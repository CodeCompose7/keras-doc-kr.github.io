---
layout: default
title: ÏΩîÎìú ÏòàÏ†ú
nav_order: 6
permalink: /examples/
has_children: true
---

* ÏõêÎ≥∏ ÎßÅÌÅ¨ : [https://keras.io/examples/](https://keras.io/examples/){:target="_blank"}
* ÏµúÏ¢Ö ÏàòÏ†ïÏùº : 2024-04-22

# ÏΩîÎìú ÏòàÏ†ú
{: .no_toc }

## Î™©Ï∞®
{: .no_toc .text-delta }

1. TOC
{:toc}

---

Code examples
=============

Our code examples are short (less than 300 lines of code), focused demonstrations of vertical deep learning workflows.

All of our examples are written as Jupyter notebooks and can be run in one click in [Google Colab](https://colab.research.google.com/notebooks/welcome.ipynb), a hosted notebook environment that requires no setup and runs in the cloud. Google Colab includes GPU and TPU runtimes.

‚òÖ
{: .label .label-purple .mx-1}
Good starter example

V3
{: .label .label-green .mx-1}
Keras 3 example

[Computer Vision]({% link docs/06-examples/01-vision.md %})
------------------------------------

### Ïù¥ÎØ∏ÏßÄ Î∂ÑÎ•ò

‚òÖ
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[Image classification from scratch]({% link docs/06-examples/01-vision/01-image_classification_from_scratch.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

‚òÖ
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[Simple MNIST convnet]({% link docs/06-examples/01-vision/02-mnist_convnet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

‚òÖ
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[Image classification via fine-tuning with EfficientNet]({% link docs/06-examples/01-vision/03-image_classification_efficientnet_fine_tuning.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image classification with Vision Transformer]({% link docs/06-examples/01-vision/04-image_classification_with_vision_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Classification using Attention-based Deep Multiple Instance Learning]({% link docs/06-examples/01-vision/05-attention_mil_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image classification with modern MLP models]({% link docs/06-examples/01-vision/06-mlp_image_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[A mobile-friendly Transformer-based model for image classification]({% link docs/06-examples/01-vision/07-mobilevit.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Pneumonia Classification on TPU]({% link docs/06-examples/01-vision/08-xray_classification_with_tpus.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Compact Convolutional Transformers]({% link docs/06-examples/01-vision/09-cct.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image classification with ConvMixer]({% link docs/06-examples/01-vision/10-convmixer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image classification with EANet (External Attention Transformer)]({% link docs/06-examples/01-vision/11-eanet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Involutional neural networks]({% link docs/06-examples/01-vision/12-involution.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image classification with Perceiver]({% link docs/06-examples/01-vision/13-perceiver_image_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Few-Shot learning with Reptile]({% link docs/06-examples/01-vision/14-reptile.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Semi-supervised image classification using contrastive pretraining with SimCLR]({% link docs/06-examples/01-vision/15-semisupervised_simclr.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image classification with Swin Transformers]({% link docs/06-examples/01-vision/16-swin_transformers.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Train a Vision Transformer on small datasets]({% link docs/06-examples/01-vision/17-vit_small_ds.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[A Vision Transformer without Attention]({% link docs/06-examples/01-vision/18-shiftvit.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image Classification using Global Context Vision Transformer]({% link docs/06-examples/01-vision/19-image_classification_using_global_context_vision_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image Classification using BigTransfer (BiT)]({% link docs/06-examples/01-vision/20-bit.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Ïù¥ÎØ∏ÏßÄ ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò

‚òÖ
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[Image segmentation with a U-Net-like architecture]({% link docs/06-examples/01-vision/21-oxford_pets_image_segmentation.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Multiclass semantic segmentation using DeepLabV3+]({% link docs/06-examples/01-vision/22-deeplabv3_plus.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Highly accurate boundaries segmentation using BASNet]({% link docs/06-examples/01-vision/23-basnet_segmentation.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image Segmentation using Composable Fully-Convolutional Networks]({% link docs/06-examples/01-vision/24-fully_convolutional_network.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Í∞ùÏ≤¥ Í∞êÏßÄ

V2
{: .label .label-yellow .mx-1}
[Object Detection with RetinaNet]({% link docs/06-examples/01-vision/25-retinanet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Keypoint Detection with Transfer Learning]({% link docs/06-examples/01-vision/26-keypoint_detection.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Object detection with Vision Transformers]({% link docs/06-examples/01-vision/27-object_detection_using_vision_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 3D

V3
{: .label .label-green .mx-1}
[3D image classification from CT scans]({% link docs/06-examples/01-vision/28-3D_image_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Monocular depth estimation]({% link docs/06-examples/01-vision/29-depth_estimation.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

‚òÖ
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[3D volumetric rendering with NeRF]({% link docs/06-examples/01-vision/30-nerf.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Point cloud segmentation with PointNet]({% link docs/06-examples/01-vision/31-pointnet_segmentation.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Point cloud classification]({% link docs/06-examples/01-vision/32-pointnet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### OCR

V3
{: .label .label-green .mx-1}
[OCR model for reading Captchas]({% link docs/06-examples/01-vision/33-captcha_ocr.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Handwriting recognition]({% link docs/06-examples/01-vision/34-handwriting_recognition.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Ïù¥ÎØ∏ÏßÄ Í∞ïÌôî

V3
{: .label .label-green .mx-1}
[Convolutional autoencoder for image denoising]({% link docs/06-examples/01-vision/35-autoencoder.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Low-light image enhancement using MIRNet]({% link docs/06-examples/01-vision/36-mirnet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image Super-Resolution using an Efficient Sub-Pixel CNN]({% link docs/06-examples/01-vision/37-super_resolution_sub_pixel.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Enhanced Deep Residual Networks for single-image super-resolution]({% link docs/06-examples/01-vision/38-edsr.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Zero-DCE for low-light image enhancement]({% link docs/06-examples/01-vision/39-zero_dce.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Îç∞Ïù¥ÌÑ∞ Î≥¥Í∞ï

V3
{: .label .label-green .mx-1}
[CutMix data augmentation for image classification]({% link docs/06-examples/01-vision/40-cutmix.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[MixUp augmentation for image classification]({% link docs/06-examples/01-vision/41-mixup.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[RandAugment for Image Classification for Improved Robustness]({% link docs/06-examples/01-vision/42-randaugment.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Ïù¥ÎØ∏ÏßÄ & ÌÖçÏä§Ìä∏

‚òÖ
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[Image captioning]({% link docs/06-examples/01-vision/43-image_captioning.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Natural language image search with a Dual Encoder]({% link docs/06-examples/01-vision/44-nl_image_search.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ÎπÑÏ†Ñ Î™®Îç∏ Ìï¥ÏÑù Í∞ÄÎä•ÏÑ±(interpretability)

V3
{: .label .label-green .mx-1}
[Visualizing what convnets learn]({% link docs/06-examples/01-vision/45-visualizing_what_convnets_learn.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Model interpretability with Integrated Gradients]({% link docs/06-examples/01-vision/46-integrated_gradients.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Investigating Vision Transformer representations]({% link docs/06-examples/01-vision/47-probing_vits.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Grad-CAM class activation visualization]({% link docs/06-examples/01-vision/48-grad_cam.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Ïù¥ÎØ∏ÏßÄ Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÉâ

V2
{: .label .label-yellow .mx-1}
[Near-duplicate image search]({% link docs/06-examples/01-vision/49-near_dup_search.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Semantic Image Clustering]({% link docs/06-examples/01-vision/50-semantic_image_clustering.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image similarity estimation using a Siamese Network with a contrastive loss]({% link docs/06-examples/01-vision/51-siamese_contrastive.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image similarity estimation using a Siamese Network with a triplet loss]({% link docs/06-examples/01-vision/52-siamese_network.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Metric learning for image similarity search]({% link docs/06-examples/01-vision/53-metric_learning.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Metric learning for image similarity search using TensorFlow Similarity]({% link docs/06-examples/01-vision/54-metric_learning_tf_similarity.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Self-supervised contrastive learning with NNCLR]({% link docs/06-examples/01-vision/55-nnclr.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ÎπÑÎîîÏò§

V3
{: .label .label-green .mx-1}
[Video Classification with a CNN-RNN Architecture]({% link docs/06-examples/01-vision/56-video_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Next-Frame Video Prediction with Convolutional LSTMs]({% link docs/06-examples/01-vision/57-conv_lstm.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Video Classification with Transformers]({% link docs/06-examples/01-vision/58-video_transformers.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Video Vision Transformer]({% link docs/06-examples/01-vision/59-vivit.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ÏÑ±Îä• Î†àÏãúÌîº

V3
{: .label .label-green .mx-1}
[Gradient Centralization for Better Training Performance]({% link docs/06-examples/01-vision/60-gradient_centralization.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Learning to tokenize in Vision Transformers]({% link docs/06-examples/01-vision/61-token_learner.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Knowledge Distillation]({% link docs/06-examples/01-vision/62-knowledge_distillation.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[FixRes: Fixing train-test resolution discrepancy]({% link docs/06-examples/01-vision/63-fixres.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Class Attention Image Transformers with LayerScale]({% link docs/06-examples/01-vision/64-cait.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Augmenting convnets with aggregated attention]({% link docs/06-examples/01-vision/65-patch_convnet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Learning to Resize]({% link docs/06-examples/01-vision/66-learnable_resizer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Í∏∞ÌÉÄ

V2
{: .label .label-yellow .mx-1}
[Semi-supervision and domain adaptation with AdaMatch]({% link docs/06-examples/01-vision/67-adamatch.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Barlow Twins for Contrastive SSL]({% link docs/06-examples/01-vision/68-barlow_twins.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Consistency training with supervision]({% link docs/06-examples/01-vision/69-consistency_training.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Distilling Vision Transformers]({% link docs/06-examples/01-vision/70-deit.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Focal Modulation: A replacement for Self-Attention]({% link docs/06-examples/01-vision/71-focal_modulation_network.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Using the Forward-Forward Algorithm for Image Classification]({% link docs/06-examples/01-vision/72-forwardforward.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Masked image modeling with Autoencoders]({% link docs/06-examples/01-vision/73-masked_image_modeling.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Segment Anything Model with ü§óTransformers]({% link docs/06-examples/01-vision/74-sam.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Semantic segmentation with SegFormer and Hugging Face Transformers]({% link docs/06-examples/01-vision/75-segformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Self-supervised contrastive learning with SimSiam]({% link docs/06-examples/01-vision/76-simsiam.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Supervised Contrastive Learning]({% link docs/06-examples/01-vision/77-supervised-contrastive-learning.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[When Recurrence meets Transformers]({% link docs/06-examples/01-vision/78-temporal_latent_bottleneck.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Efficient Object Detection with YOLOV8 and KerasCV]({% link docs/06-examples/01-vision/79-yolov8.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

* * *

[Natural Language Processing]({% link docs/06-examples/02-nlp.md %})
---------------------------------------------

### ÌÖçÏä§Ìä∏ Î∂ÑÎ•ò

‚òÖ
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[Text classification from scratch]({% link docs/06-examples/02-nlp/01-text_classification_from_scratch.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Review Classification using Active Learning]({% link docs/06-examples/02-nlp/02-active_learning_review_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Text Classification using FNet]({% link docs/06-examples/02-nlp/03-fnet_classification_with_keras_nlp.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Large-scale multi-label text classification]({% link docs/06-examples/02-nlp/04-multi_label_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Text classification with Transformer]({% link docs/06-examples/02-nlp/05-text_classification_with_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Text classification with Switch Transformer]({% link docs/06-examples/02-nlp/06-text_classification_with_switch_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Text classification using Decision Forests and pretrained embeddings]({% link docs/06-examples/02-nlp/07-tweet-classification-using-tfdf.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Using pre-trained word embeddings]({% link docs/06-examples/02-nlp/08-pretrained_word_embeddings.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Bidirectional LSTM on IMDB]({% link docs/06-examples/02-nlp/09-bidirectional_lstm_imdb.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Data Parallel Training with KerasNLP and tf.distribute]({% link docs/06-examples/02-nlp/10-data_parallel_training_with_keras_nlp.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Í∏∞Í≥Ñ Î≤àÏó≠

V3
{: .label .label-green .mx-1}
[English-to-Spanish translation with KerasNLP]({% link docs/06-examples/02-nlp/11-neural_machine_translation_with_keras_nlp.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

‚òÖ
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[English-to-Spanish translation with a sequence-to-sequence Transformer]({% link docs/06-examples/02-nlp/12-neural_machine_translation_with_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Character-level recurrent sequence-to-sequence model]({% link docs/06-examples/02-nlp/13-lstm_seq2seq.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Ïó∞Í¥ÄÏÑ± ÏòàÏ∏° (Entailment prediction)

V2
{: .label .label-yellow .mx-1}
[Multimodal entailment]({% link docs/06-examples/02-nlp/14-multimodal_entailment.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Î™ÖÎ™ÖÎêú ÏóîÌã∞Ìã∞ Ïù∏Ïãù

V3
{: .label .label-green .mx-1}
[Named Entity Recognition using Transformers]({% link docs/06-examples/02-nlp/15-ner_transformers.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Sequence-to-sequence

V2
{: .label .label-yellow .mx-1}
[Text Extraction with BERT]({% link docs/06-examples/02-nlp/16-text_extraction_with_bert.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Sequence to sequence learning for performing number addition]({% link docs/06-examples/02-nlp/17-addition_rnn.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ÌÖçÏä§Ìä∏ Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÉâ

V3
{: .label .label-green .mx-1}
[Semantic Similarity with KerasNLP]({% link docs/06-examples/02-nlp/18-semantic_similarity_with_keras_nlp.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Semantic Similarity with BERT]({% link docs/06-examples/02-nlp/19-semantic_similarity_with_bert.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Sentence embeddings using Siamese RoBERTa-networks]({% link docs/06-examples/02-nlp/20-sentence_embeddings_with_sbert.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Ïñ∏Ïñ¥ Î™®Îç∏ÎßÅ

V2
{: .label .label-yellow .mx-1}
[End-to-end Masked Language Modeling with BERT]({% link docs/06-examples/02-nlp/21-masked_language_modeling.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Pretraining BERT with Hugging Face Transformers]({% link docs/06-examples/02-nlp/22-pretraining_BERT.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Ìö®Ïú®Ï†ÅÏù∏ Îß§Í∞úÎ≥ÄÏàò ÎØ∏ÏÑ∏ Ï°∞Ï†ï

V3
{: .label .label-green .mx-1}
[Parameter-efficient fine-tuning of GPT-2 with LoRA]({% link docs/06-examples/02-nlp/23-parameter_efficient_finetuning_of_gpt2_with_lora.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Í∏∞ÌÉÄ

V2
{: .label .label-yellow .mx-1}
[Abstractive Text Summarization with BART]({% link docs/06-examples/02-nlp/24-abstractive_summarization_with_bart.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Training a language model from scratch with ü§ó Transformers and TPUs]({% link docs/06-examples/02-nlp/25-mlm_training_tpus.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[MultipleChoice Task with Transfer Learning]({% link docs/06-examples/02-nlp/26-multiple_choice_task_with_transfer_learning.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Question Answering with Hugging Face Transformers]({% link docs/06-examples/02-nlp/27-question_answering.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Abstractive Summarization with Hugging Face Transformers]({% link docs/06-examples/02-nlp/28-t5_hf_summarization.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

* * *

[Structured Data]({% link docs/06-examples/03-structured_data.md %})
---------------------------------------------

### Íµ¨Ï°∞ÌôîÎêú Îç∞Ïù¥ÌÑ∞ Î∂ÑÎ•ò

‚òÖ
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[Structured data classification with FeatureSpace]({% link docs/06-examples/03-structured_data/01-structured_data_classification_with_feature_space.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

‚òÖ
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[Imbalanced classification: credit card fraud detection]({% link docs/06-examples/03-structured_data/02-imbalanced_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Structured data classification from scratch]({% link docs/06-examples/03-structured_data/03-structured_data_classification_from_scratch.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Structured data learning with Wide, Deep, and Cross networks]({% link docs/06-examples/03-structured_data/04-wide_deep_cross_networks.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Classification with Gated Residual and Variable Selection Networks]({% link docs/06-examples/03-structured_data/05-classification_with_grn_and_vsn.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Classification with TensorFlow Decision Forests]({% link docs/06-examples/03-structured_data/06-classification_with_tfdf.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Classification with Neural Decision Forests]({% link docs/06-examples/03-structured_data/07-deep_neural_decision_forests.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Structured data learning with TabTransformer]({% link docs/06-examples/03-structured_data/08-tabtransformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Ï∂îÏ≤ú

V3
{: .label .label-green .mx-1}
[Collaborative Filtering for Movie Recommendations]({% link docs/06-examples/03-structured_data/09-collaborative_filtering_movielens.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[A Transformer-based recommendation system]({% link docs/06-examples/03-structured_data/10-movielens_recommendations_transformers.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Í∏∞ÌÉÄ

V2
{: .label .label-yellow .mx-1}
[FeatureSpace advanced use cases]({% link docs/06-examples/03-structured_data/11-feature_space_advanced.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

* * *

[Timeseries]({% link docs/06-examples/04-timeseries.md %})
-----------------------------------

### ÌÉÄÏûÑÏãúÎ¶¨Ï¶à Î∂ÑÎ•ò

‚òÖ
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[Timeseries classification from scratch]({% link docs/06-examples/04-timeseries/01-timeseries_classification_from_scratch.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Timeseries classification with a Transformer model]({% link docs/06-examples/04-timeseries/02-timeseries_classification_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Electroencephalogram Signal Classification for action identification]({% link docs/06-examples/04-timeseries/03-eeg_signal_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Event classification for payment card fraud detection]({% link docs/06-examples/04-timeseries/04-event_classification_for_payment_card_fraud_detection.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Ïù¥ÏÉÅ ÏßïÌõÑ ÌÉêÏßÄ

V3
{: .label .label-green .mx-1}
[Timeseries anomaly detection using an Autoencoder]({% link docs/06-examples/04-timeseries/05-timeseries_anomaly_detection.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ÌÉÄÏûÑÏãúÎ¶¨Ï¶à ÏòàÏ∏°

V3
{: .label .label-green .mx-1}
[Traffic forecasting using graph neural networks and LSTM]({% link docs/06-examples/04-timeseries/06-timeseries_traffic_forecasting.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Timeseries forecasting for weather prediction]({% link docs/06-examples/04-timeseries/07-timeseries_weather_forecasting.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

* * *

[Generative Deep Learning]({% link docs/06-examples/05-generative.md %})
-------------------------------------------------

### Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ±

‚òÖ
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[Denoising Diffusion Implicit Models]({% link docs/06-examples/05-generative/01-ddim.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

‚òÖ
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[A walk through latent space with Stable Diffusion]({% link docs/06-examples/05-generative/02-random_walks_with_stable_diffusion.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[DreamBooth]({% link docs/06-examples/05-generative/03-dreambooth.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Denoising Diffusion Probabilistic Models]({% link docs/06-examples/05-generative/04-ddpm.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Teach StableDiffusion new concepts via Textual Inversion]({% link docs/06-examples/05-generative/05-fine_tune_via_textual_inversion.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Fine-tuning Stable Diffusion]({% link docs/06-examples/05-generative/06-finetune_stable_diffusion.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Variational AutoEncoder]({% link docs/06-examples/05-generative/07-vae.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[GAN overriding Model.train\_step]({% link docs/06-examples/05-generative/08-dcgan_overriding_train_step.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[WGAN-GP overriding Model.train\_step]({% link docs/06-examples/05-generative/09-wgan_gp.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Conditional GAN]({% link docs/06-examples/05-generative/10-conditional_gan.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[CycleGAN]({% link docs/06-examples/05-generative/11-cyclegan.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Data-efficient GANs with Adaptive Discriminator Augmentation]({% link docs/06-examples/05-generative/12-gan_ada.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Deep Dream]({% link docs/06-examples/05-generative/13-deep_dream.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[GauGAN for conditional image generation]({% link docs/06-examples/05-generative/14-gaugan.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[PixelCNN]({% link docs/06-examples/05-generative/15-pixelcnn.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Face image generation with StyleGAN]({% link docs/06-examples/05-generative/16-stylegan.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Vector-Quantized Variational Autoencoders]({% link docs/06-examples/05-generative/17-vq_vae.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

### Ïä§ÌÉÄÏùº Ï†ÑÏù¥

V3
{: .label .label-green .mx-1}
[Neural style transfer]({% link docs/06-examples/05-generative/18-neural_style_transfer.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Neural Style Transfer with AdaIN]({% link docs/06-examples/05-generative/19-adain.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

### ÌÖçÏä§Ìä∏ ÏÉùÏÑ±

‚òÖ
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[GPT2 Text Generation with KerasNLP]({% link docs/06-examples/05-generative/20-gpt2_text_generation_with_kerasnlp.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[GPT text generation from scratch with KerasNLP]({% link docs/06-examples/05-generative/21-text_generation_gpt.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Text generation with a miniature GPT]({% link docs/06-examples/05-generative/22-text_generation_with_miniature_gpt.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Character-level text generation with LSTM]({% link docs/06-examples/05-generative/23-lstm_character_level_text_generation.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Text Generation using FNet]({% link docs/06-examples/05-generative/24-text_generation_fnet.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

### Í∑∏ÎûòÌîÑ ÏÉùÏÑ±

V2
{: .label .label-yellow .mx-1}
[Drug Molecule Generation with VAE]({% link docs/06-examples/05-generative/25-molecule_generation.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[WGAN-GP with R-GCN for the generation of small molecular graphs]({% link docs/06-examples/05-generative/26-wgan-graphs.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

### Í∏∞ÌÉÄ

V2
{: .label .label-yellow .mx-1}
[Density estimation using Real NVP]({% link docs/06-examples/05-generative/27-real_nvp.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

* * *

[Audio Data]({% link docs/06-examples/06-audio.md %})
------------------------------

### ÏùåÏÑ± Ïù∏Ïãù

V3
{: .label .label-green .mx-1}
[Automatic Speech Recognition with Transformer]({% link docs/06-examples/06-audio/01-transformer_asr.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

### Í∏∞ÌÉÄ

V2
{: .label .label-yellow .mx-1}
[Automatic Speech Recognition using CTC]({% link docs/06-examples/06-audio/02-ctc_asr.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[MelGAN-based spectrogram inversion using feature matching]({% link docs/06-examples/06-audio/03-melgan_spectrogram_inversion.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Speaker Recognition]({% link docs/06-examples/06-audio/04-speaker_recognition_using_cnn.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[English speaker accent recognition using Transfer Learning]({% link docs/06-examples/06-audio/05-uk_ireland_accent_recognition.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Audio Classification with Hugging Face Transformers]({% link docs/06-examples/06-audio/06-wav2vec2_audiocls.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

* * *

[Reinforcement Learning]({% link docs/06-examples/07-rl.md %})
---------------------------------------

### RL ÏïåÍ≥†Î¶¨Ï¶ò

V3
{: .label .label-green .mx-1}
[Actor Critic Method]({% link docs/06-examples/07-rl/01-actor_critic_cartpole.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Proximal Policy Optimization]({% link docs/06-examples/07-rl/02-ppo_cartpole.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Deep Q-Learning for Atari Breakout]({% link docs/06-examples/07-rl/03-deep_q_network_breakout.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

### Í∏∞ÌÉÄ

V2
{: .label .label-yellow .mx-1}
[Deep Deterministic Policy Gradient (DDPG)]({% link docs/06-examples/07-rl/04-ddpg_pendulum.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

* * *

[Graph Data]({% link docs/06-examples/08-graph.md %})
------------------------------

### Í∑∏ÎûòÌîÑ Îç∞Ïù¥ÌÑ∞ (Graph Data)

V2
{: .label .label-yellow .mx-1}
[Graph attention network (GAT) for node classification]({% link docs/06-examples/08-graph/01-gat_node_classification.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Node Classification with Graph Neural Networks]({% link docs/06-examples/08-graph/02-gnn_citations.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Message-passing neural network (MPNN) for molecular property prediction]({% link docs/06-examples/08-graph/03-mpnn-molecular-graphs.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Graph representation learning with node2vec]({% link docs/06-examples/08-graph/04-node2vec_movielens.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

* * *

[Quick Keras Recipes]({% link docs/06-examples/09-keras_recipes.md %})
-----------------------------------------------

### ÏÑúÎπÑÏä§

V3
{: .label .label-green .mx-1}
[Serving TensorFlow models with TFServing]({% link docs/06-examples/09-keras_recipes/01-tf_serving.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

### Keras ÏÇ¨Ïö© ÌåÅ

V3
{: .label .label-green .mx-1}
[Keras debugging tips]({% link docs/06-examples/09-keras_recipes/02-debugging_tips.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Customizing the convolution operation of a Conv2D layer]({% link docs/06-examples/09-keras_recipes/03-subclassing_conv_layers.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Trainer pattern]({% link docs/06-examples/09-keras_recipes/04-trainer_pattern.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Endpoint layer pattern]({% link docs/06-examples/09-keras_recipes/05-endpoint_layer_pattern.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Reproducibility in Keras Models]({% link docs/06-examples/09-keras_recipes/06-reproducibility_recipes.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Writing Keras Models With TensorFlow NumPy]({% link docs/06-examples/09-keras_recipes/07-tensorflow_numpy_models.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Simple custom layer example: Antirectifier]({% link docs/06-examples/09-keras_recipes/08-antirectifier.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Packaging Keras models for wide distribution using Functional Subclassing]({% link docs/06-examples/09-keras_recipes/09-packaging_keras_models_for_wide_distribution.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

### ML Î™®Î≤î ÏÇ¨Î°Ä

V3
{: .label .label-green .mx-1}
[Estimating required sample size for model training]({% link docs/06-examples/09-keras_recipes/10-sample_size_estimate.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Memory-efficient embeddings for recommendation systems]({% link docs/06-examples/09-keras_recipes/11-memory_efficient_embeddings.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Creating TFRecords]({% link docs/06-examples/09-keras_recipes/12-creating_tfrecords.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

### Í∏∞ÌÉÄ

V2
{: .label .label-yellow .mx-1}
[Approximating non-Function Mappings with Mixture Density Networks]({% link docs/06-examples/09-keras_recipes/13-approximating_non_function_mappings.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Probabilistic Bayesian Neural Networks]({% link docs/06-examples/09-keras_recipes/14-bayesian_neural_networks.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Knowledge distillation recipes]({% link docs/06-examples/09-keras_recipes/15-better_knowledge_distillation.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Evaluating and exporting scikit-learn metrics in a Keras callback]({% link docs/06-examples/09-keras_recipes/16-sklearn_metric_callbacks.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[How to train a Keras model on TFRecord files]({% link docs/06-examples/09-keras_recipes/17-tfrecord.md %})
{: .d-inline .v-align-middle}
.
{: .lh-0 .my-0 .opacity-0}

* * *

Adding a new code example
-------------------------

We welcome new code examples! Here are our rules:

*   They should be shorter than 300 lines of code (comments may be as long as you want).
*   They should demonstrate modern Keras best practices.
*   They should be substantially different in topic from all examples listed above.
*   They should be extensively documented & commented.

New examples are added via Pull Requests to the [keras.io repository](https://github.com/keras-team/keras-io). They must be submitted as a `.py` file that follows a specific format. They are usually generated from Jupyter notebooks. See the [`tutobooks` documentation](https://github.com/keras-team/keras-io/blob/master/README.md) for more details.

If you would like to convert a Keras 2 example to Keras 3, please open a Pull Request to the [keras.io repository](https://github.com/keras-team/keras-io).
