---
layout: default
title: Ïª¥Ìì®ÌÑ∞ ÎπÑÏ†Ñ
nav_order: 1
permalink: /examples/vision/
parent: ÏΩîÎìú ÏòàÏ†ú
has_children: true
---

* ÏõêÎ≥∏ ÎßÅÌÅ¨ : [https://keras.io/examples/vision/](https://keras.io/examples/vision/){:target="_blank"}
* ÏµúÏ¢Ö ÏàòÏ†ïÏùº : 2024-03-31

# Ïª¥Ìì®ÌÑ∞ ÎπÑÏ†Ñ
{: .no_toc }

## Î™©Ï∞®
{: .no_toc .text-delta }

1. TOC
{:toc}

---

### Ïù¥ÎØ∏ÏßÄ Î∂ÑÎ•ò

‚òÖ
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[Image classification from scratch](/examples/vision/image_classification_from_scratch)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

‚òÖ
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[Simple MNIST convnet](/examples/vision/mnist_convnet) 
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

‚òÖ
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[Image classification via fine-tuning with EfficientNet](/examples/vision/image_classification_efficientnet_fine_tuning)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image classification with Vision Transformer](/examples/vision/image_classification_with_vision_transformer)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Classification using Attention-based Deep Multiple Instance Learning](/examples/vision/attention_mil_classification)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image classification with modern MLP models](/examples/vision/mlp_image_classification)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[A mobile-friendly Transformer-based model for image classification](/examples/vision/mobilevit)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Pneumonia Classification on TPU](/examples/vision/xray_classification_with_tpus)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Compact Convolutional Transformers](/examples/vision/cct)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image classification with ConvMixer](/examples/vision/convmixer)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image classification with EANet (External Attention Transformer)](/examples/vision/eanet)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Involutional neural networks](/examples/vision/involution)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image classification with Perceiver](/examples/vision/perceiver_image_classification)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Few-Shot learning with Reptile](/examples/vision/reptile)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Semi-supervised image classification using contrastive pretraining with SimCLR](/examples/vision/semisupervised_simclr)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image classification with Swin Transformers](/examples/vision/swin_transformers)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Train a Vision Transformer on small datasets](/examples/vision/vit_small_ds)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[A Vision Transformer without Attention](/examples/vision/shiftvit)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image Classification using Global Context Vision Transformer](/examples/vision/image_classification_using_global_context_vision_transformer)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image Classification using BigTransfer (BiT)](/examples/vision/bit)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Ïù¥ÎØ∏ÏßÄ ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò

‚òÖ
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[Image segmentation with a U-Net-like architecture](/examples/vision/oxford_pets_image_segmentation)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Multiclass semantic segmentation using DeepLabV3+](/examples/vision/deeplabv3_plus)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Highly accurate boundaries segmentation using BASNet](/examples/vision/basnet_segmentation)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image Segmentation using Composable Fully-Convolutional Networks](/examples/vision/fully_convolutional_network)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Í∞ùÏ≤¥ Í∞êÏßÄ

V2
{: .label .label-yellow .mx-1}
[Object Detection with RetinaNet](/examples/vision/retinanet)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Keypoint Detection with Transfer Learning](/examples/vision/keypoint_detection)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Object detection with Vision Transformers](/examples/vision/object_detection_using_vision_transformer)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 3D

V3
{: .label .label-green .mx-1}
[3D image classification from CT scans](/examples/vision/3D_image_classification)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Monocular depth estimation](/examples/vision/depth_estimation)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[3D volumetric rendering with NeRF](/examples/vision/nerf)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Point cloud segmentation with PointNet](/examples/vision/pointnet_segmentation)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Point cloud classification](/examples/vision/pointnet)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### OCR

[

**V3**

OCR model for reading Captchas



](/examples/vision/captcha_ocr)[

V2

Handwriting recognition



](/examples/vision/handwriting_recognition)

### Image enhancement

[

**V3**

Convolutional autoencoder for image denoising



](/examples/vision/autoencoder)[

**V3**

Low-light image enhancement using MIRNet



](/examples/vision/mirnet)[

**V3**

Image Super-Resolution using an Efficient Sub-Pixel CNN



](/examples/vision/super_resolution_sub_pixel)[

V2

Enhanced Deep Residual Networks for single-image super-resolution



](/examples/vision/edsr)[

**V3**

Zero-DCE for low-light image enhancement



](/examples/vision/zero_dce)

### Data augmentation

[

**V3**

CutMix data augmentation for image classification



](/examples/vision/cutmix)[

**V3**

MixUp augmentation for image classification



](/examples/vision/mixup)[

**V3**

RandAugment for Image Classification for Improved Robustness



](/examples/vision/randaugment)

### Image & Text

[

‚òÖ

**V3**

Image captioning



](/examples/vision/image_captioning)[

V2

Natural language image search with a Dual Encoder



](/examples/vision/nl_image_search)

### Vision models interpretability

[

**V3**

Visualizing what convnets learn



](/examples/vision/visualizing_what_convnets_learn)[

**V3**

Model interpretability with Integrated Gradients



](/examples/vision/integrated_gradients)[

**V3**

Investigating Vision Transformer representations



](/examples/vision/probing_vits)[

**V3**

Grad-CAM class activation visualization



](/examples/vision/grad_cam)

### Image similarity search

[

V2

Near-duplicate image search



](/examples/vision/near_dup_search)[

**V3**

Semantic Image Clustering



](/examples/vision/semantic_image_clustering)[

**V3**

Image similarity estimation using a Siamese Network with a contrastive loss



](/examples/vision/siamese_contrastive)[

**V3**

Image similarity estimation using a Siamese Network with a triplet loss



](/examples/vision/siamese_network)[

**V3**

Metric learning for image similarity search



](/examples/vision/metric_learning)[

V2

Metric learning for image similarity search using TensorFlow Similarity



](/examples/vision/metric_learning_tf_similarity)[

**V3**

Self-supervised contrastive learning with NNCLR



](/examples/vision/nnclr)

### Video

[

**V3**

Video Classification with a CNN-RNN Architecture



](/examples/vision/video_classification)[

**V3**

Next-Frame Video Prediction with Convolutional LSTMs



](/examples/vision/conv_lstm)[

**V3**

Video Classification with Transformers



](/examples/vision/video_transformers)[

**V3**

Video Vision Transformer



](/examples/vision/vivit)

### Performance recipes

[

**V3**

Gradient Centralization for Better Training Performance



](/examples/vision/gradient_centralization)[

**V3**

Learning to tokenize in Vision Transformers



](/examples/vision/token_learner)[

**V3**

Knowledge Distillation



](/examples/vision/knowledge_distillation)[

**V3**

FixRes: Fixing train-test resolution discrepancy



](/examples/vision/fixres)[

**V3**

Class Attention Image Transformers with LayerScale



](/examples/vision/cait)[

**V3**

Augmenting convnets with aggregated attention



](/examples/vision/patch_convnet)[

**V3**

Learning to Resize



](/examples/vision/learnable_resizer)

### Other

[

V2

Semi-supervision and domain adaptation with AdaMatch



](/examples/vision/adamatch)[

V2

Barlow Twins for Contrastive SSL



](/examples/vision/barlow_twins)[

V2

Consistency training with supervision



](/examples/vision/consistency_training)[

V2

Distilling Vision Transformers



](/examples/vision/deit)[

V2

Focal Modulation: A replacement for Self-Attention



](/examples/vision/focal_modulation_network)[

V2

Using the Forward-Forward Algorithm for Image Classification



](/examples/vision/forwardforward)[

V2

Masked image modeling with Autoencoders



](/examples/vision/masked_image_modeling)[

V2

Segment Anything Model with ü§óTransformers



](/examples/vision/sam)[

V2

Semantic segmentation with SegFormer and Hugging Face Transformers



](/examples/vision/segformer)[

V2

Self-supervised contrastive learning with SimSiam



](/examples/vision/simsiam)[

V2

Supervised Contrastive Learning



](/examples/vision/supervised-contrastive-learning)[

V2

When Recurrence meets Transformers



](/examples/vision/temporal_latent_bottleneck)[

V2

Efficient Object Detection with YOLOV8 and KerasCV



](/examples/vision/yolov8)

* * *