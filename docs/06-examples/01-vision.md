---
layout: default
title: Ïª¥Ìì®ÌÑ∞ ÎπÑÏ†Ñ
nav_order: 1
permalink: /examples/vision/
parent: ÏΩîÎìú ÏòàÏ†ú
has_children: true
---

* ÏõêÎ≥∏ ÎßÅÌÅ¨ : [https://keras.io/examples/vision/](https://keras.io/examples/vision/){:target="_blank"}
* ÏµúÏ¢Ö ÏàòÏ†ïÏùº : 2024-04-01

# Ïª¥Ìì®ÌÑ∞ ÎπÑÏ†Ñ
{: .no_toc }

## Î™©Ï∞®
{: .no_toc .text-delta }

1. TOC
{:toc}

---

### Ïù¥ÎØ∏ÏßÄ Î∂ÑÎ•ò

‚òÖ
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[Image classification from scratch]({% link docs/06-examples/01-vision/01-image_classification_from_scratch.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

‚òÖ
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[Simple MNIST convnet]({% link docs/06-examples/01-vision/02-mnist_convnet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

‚òÖ
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[Image classification via fine-tuning with EfficientNet]({% link docs/06-examples/01-vision/03-image_classification_efficientnet_fine_tuning.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image classification with Vision Transformer]({% link docs/06-examples/01-vision/04-image_classification_with_vision_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Classification using Attention-based Deep Multiple Instance Learning]({% link docs/06-examples/01-vision/05-attention_mil_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image classification with modern MLP models]({% link docs/06-examples/01-vision/06-mlp_image_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[A mobile-friendly Transformer-based model for image classification]({% link docs/06-examples/01-vision/07-mobilevit.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Pneumonia Classification on TPU]({% link docs/06-examples/01-vision/08-xray_classification_with_tpus.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Compact Convolutional Transformers]({% link docs/06-examples/01-vision/09-cct.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image classification with ConvMixer]({% link docs/06-examples/01-vision/10-convmixer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image classification with EANet (External Attention Transformer)]({% link docs/06-examples/01-vision/11-eanet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Involutional neural networks]({% link docs/06-examples/01-vision/12-involution.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image classification with Perceiver]({% link docs/06-examples/01-vision/13-perceiver_image_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Few-Shot learning with Reptile]({% link docs/06-examples/01-vision/14-reptile.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Semi-supervised image classification using contrastive pretraining with SimCLR]({% link docs/06-examples/01-vision/15-semisupervised_simclr.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image classification with Swin Transformers]({% link docs/06-examples/01-vision/16-swin_transformers.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Train a Vision Transformer on small datasets]({% link docs/06-examples/01-vision/17-vit_small_ds.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[A Vision Transformer without Attention]({% link docs/06-examples/01-vision/18-shiftvit.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image Classification using Global Context Vision Transformer]({% link docs/06-examples/01-vision/19-image_classification_using_global_context_vision_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image Classification using BigTransfer (BiT)]({% link docs/06-examples/01-vision/20-bit.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Ïù¥ÎØ∏ÏßÄ ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò

‚òÖ
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[Image segmentation with a U-Net-like architecture]({% link docs/06-examples/01-vision/21-oxford_pets_image_segmentation.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Multiclass semantic segmentation using DeepLabV3+]({% link docs/06-examples/01-vision/22-deeplabv3_plus.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Highly accurate boundaries segmentation using BASNet]({% link docs/06-examples/01-vision/23-basnet_segmentation.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image Segmentation using Composable Fully-Convolutional Networks]({% link docs/06-examples/01-vision/24-fully_convolutional_network.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Í∞ùÏ≤¥ Í∞êÏßÄ

V2
{: .label .label-yellow .mx-1}
[Object Detection with RetinaNet]({% link docs/06-examples/01-vision/25-retinanet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Keypoint Detection with Transfer Learning]({% link docs/06-examples/01-vision/26-keypoint_detection.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Object detection with Vision Transformers]({% link docs/06-examples/01-vision/27-object_detection_using_vision_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 3D

V3
{: .label .label-green .mx-1}
[3D image classification from CT scans]({% link docs/06-examples/01-vision/28-3D_image_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Monocular depth estimation]({% link docs/06-examples/01-vision/29-depth_estimation.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

‚òÖ
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[3D volumetric rendering with NeRF]({% link docs/06-examples/01-vision/30-nerf.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Point cloud segmentation with PointNet]({% link docs/06-examples/01-vision/31-pointnet_segmentation.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Point cloud classification]({% link docs/06-examples/01-vision/32-pointnet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### OCR

V3
{: .label .label-green .mx-1}
[OCR model for reading Captchas]({% link docs/06-examples/01-vision/33-captcha_ocr.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Handwriting recognition]({% link docs/06-examples/01-vision/34-handwriting_recognition.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Ïù¥ÎØ∏ÏßÄ Í∞ïÌôî

V3
{: .label .label-green .mx-1}
[Convolutional autoencoder for image denoising]({% link docs/06-examples/01-vision/35-autoencoder.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Low-light image enhancement using MIRNet]({% link docs/06-examples/01-vision/36-mirnet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image Super-Resolution using an Efficient Sub-Pixel CNN]({% link docs/06-examples/01-vision/37-super_resolution_sub_pixel.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Enhanced Deep Residual Networks for single-image super-resolution]({% link docs/06-examples/01-vision/38-edsr.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Zero-DCE for low-light image enhancement]({% link docs/06-examples/01-vision/39-zero_dce.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Îç∞Ïù¥ÌÑ∞ Î≥¥Í∞ï

V3
{: .label .label-green .mx-1}
[CutMix data augmentation for image classification]({% link docs/06-examples/01-vision/40-cutmix.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[MixUp augmentation for image classification]({% link docs/06-examples/01-vision/41-mixup.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[RandAugment for Image Classification for Improved Robustness]({% link docs/06-examples/01-vision/42-randaugment.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Ïù¥ÎØ∏ÏßÄ & ÌÖçÏä§Ìä∏

‚òÖ
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[Image captioning]({% link docs/06-examples/01-vision/43-image_captioning.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Natural language image search with a Dual Encoder]({% link docs/06-examples/01-vision/44-nl_image_search.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ÎπÑÏ†Ñ Î™®Îç∏ Ìï¥ÏÑù Í∞ÄÎä•ÏÑ±(interpretability)

V3
{: .label .label-green .mx-1}
[Visualizing what convnets learn]({% link docs/06-examples/01-vision/45-visualizing_what_convnets_learn.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Model interpretability with Integrated Gradients]({% link docs/06-examples/01-vision/46-integrated_gradients.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Investigating Vision Transformer representations]({% link docs/06-examples/01-vision/47-probing_vits.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Grad-CAM class activation visualization]({% link docs/06-examples/01-vision/48-grad_cam.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Ïù¥ÎØ∏ÏßÄ Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÉâ

V2
{: .label .label-yellow .mx-1}
[Near-duplicate image search]({% link docs/06-examples/01-vision/49-near_dup_search.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Semantic Image Clustering]({% link docs/06-examples/01-vision/50-semantic_image_clustering.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image similarity estimation using a Siamese Network with a contrastive loss]({% link docs/06-examples/01-vision/51-siamese_contrastive.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image similarity estimation using a Siamese Network with a triplet loss]({% link docs/06-examples/01-vision/52-siamese_network.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Metric learning for image similarity search]({% link docs/06-examples/01-vision/53-metric_learning.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Metric learning for image similarity search using TensorFlow Similarity]({% link docs/06-examples/01-vision/54-metric_learning_tf_similarity.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Self-supervised contrastive learning with NNCLR]({% link docs/06-examples/01-vision/55-nnclr.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ÎπÑÎîîÏò§

V3
{: .label .label-green .mx-1}
[Video Classification with a CNN-RNN Architecture]({% link docs/06-examples/01-vision/56-video_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Next-Frame Video Prediction with Convolutional LSTMs]({% link docs/06-examples/01-vision/57-conv_lstm.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Video Classification with Transformers]({% link docs/06-examples/01-vision/58-video_transformers.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Video Vision Transformer]({% link docs/06-examples/01-vision/59-vivit.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ÏÑ±Îä• Î†àÏãúÌîº

V3
{: .label .label-green .mx-1}
[Gradient Centralization for Better Training Performance]({% link docs/06-examples/01-vision/60-gradient_centralization.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Learning to tokenize in Vision Transformers]({% link docs/06-examples/01-vision/61-token_learner.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Knowledge Distillation]({% link docs/06-examples/01-vision/62-knowledge_distillation.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[FixRes: Fixing train-test resolution discrepancy]({% link docs/06-examples/01-vision/63-fixres.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Class Attention Image Transformers with LayerScale]({% link docs/06-examples/01-vision/64-cait.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Augmenting convnets with aggregated attention]({% link docs/06-examples/01-vision/65-patch_convnet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Learning to Resize]({% link docs/06-examples/01-vision/66-learnable_resizer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Í∏∞ÌÉÄ

V2
{: .label .label-yellow .mx-1}
[Semi-supervision and domain adaptation with AdaMatch]({% link docs/06-examples/01-vision/67-adamatch.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Barlow Twins for Contrastive SSL]({% link docs/06-examples/01-vision/68-barlow_twins.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Consistency training with supervision]({% link docs/06-examples/01-vision/69-consistency_training.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Distilling Vision Transformers]({% link docs/06-examples/01-vision/70-deit.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Focal Modulation: A replacement for Self-Attention]({% link docs/06-examples/01-vision/71-focal_modulation_network.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Using the Forward-Forward Algorithm for Image Classification]({% link docs/06-examples/01-vision/72-forwardforward.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Masked image modeling with Autoencoders]({% link docs/06-examples/01-vision/73-masked_image_modeling.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Segment Anything Model with ü§óTransformers]({% link docs/06-examples/01-vision/74-sam.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Semantic segmentation with SegFormer and Hugging Face Transformers]({% link docs/06-examples/01-vision/75-segformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Self-supervised contrastive learning with SimSiam]({% link docs/06-examples/01-vision/76-simsiam.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Supervised Contrastive Learning]({% link docs/06-examples/01-vision/77-supervised-contrastive-learning.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[When Recurrence meets Transformers]({% link docs/06-examples/01-vision/78-temporal_latent_bottleneck.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Efficient Object Detection with YOLOV8 and KerasCV]({% link docs/06-examples/01-vision/79-yolov8.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}
