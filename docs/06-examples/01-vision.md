---
layout: default
title: 컴퓨터 비전
nav_order: 1
permalink: /examples/vision/
parent: 코드 예제
has_children: true
---

* 원본 링크 : [https://keras.io/examples/vision/](https://keras.io/examples/vision/){:target="_blank"}
* 최종 수정일 : 2024-03-31

# 컴퓨터 비전
{: .no_toc }

## 목차
{: .no_toc .text-delta }

1. TOC
{:toc}

---

### 이미지 분류

★
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[Image classification from scratch](/examples/vision/image_classification_from_scratch)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

★
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[Simple MNIST convnet](/examples/vision/mnist_convnet) 
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

★
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[Image classification via fine-tuning with EfficientNet](/examples/vision/image_classification_efficientnet_fine_tuning)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image classification with Vision Transformer](/examples/vision/image_classification_with_vision_transformer)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Classification using Attention-based Deep Multiple Instance Learning](/examples/vision/attention_mil_classification)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image classification with modern MLP models](/examples/vision/mlp_image_classification)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[A mobile-friendly Transformer-based model for image classification](/examples/vision/mobilevit)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Pneumonia Classification on TPU](/examples/vision/xray_classification_with_tpus)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Compact Convolutional Transformers](/examples/vision/cct)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image classification with ConvMixer](/examples/vision/convmixer)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image classification with EANet (External Attention Transformer)](/examples/vision/eanet)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Involutional neural networks](/examples/vision/involution)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image classification with Perceiver](/examples/vision/perceiver_image_classification)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Few-Shot learning with Reptile](/examples/vision/reptile)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Semi-supervised image classification using contrastive pretraining with SimCLR](/examples/vision/semisupervised_simclr)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image classification with Swin Transformers](/examples/vision/swin_transformers)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Train a Vision Transformer on small datasets](/examples/vision/vit_small_ds)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[A Vision Transformer without Attention](/examples/vision/shiftvit)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image Classification using Global Context Vision Transformer](/examples/vision/image_classification_using_global_context_vision_transformer)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image Classification using BigTransfer (BiT)](/examples/vision/bit)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 이미지 세그멘테이션

★
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[Image segmentation with a U-Net-like architecture](/examples/vision/oxford_pets_image_segmentation)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Multiclass semantic segmentation using DeepLabV3+](/examples/vision/deeplabv3_plus)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Highly accurate boundaries segmentation using BASNet](/examples/vision/basnet_segmentation)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image Segmentation using Composable Fully-Convolutional Networks](/examples/vision/fully_convolutional_network)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 객체 감지

V2
{: .label .label-yellow .mx-1}
[Object Detection with RetinaNet](/examples/vision/retinanet)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Keypoint Detection with Transfer Learning](/examples/vision/keypoint_detection)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Object detection with Vision Transformers](/examples/vision/object_detection_using_vision_transformer)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 3D

V3
{: .label .label-green .mx-1}
[3D image classification from CT scans](/examples/vision/3D_image_classification)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Monocular depth estimation](/examples/vision/depth_estimation)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

★
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[3D volumetric rendering with NeRF](/examples/vision/nerf)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Point cloud segmentation with PointNet](/examples/vision/pointnet_segmentation)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Point cloud classification](/examples/vision/pointnet)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### OCR

V3
{: .label .label-green .mx-1}
[OCR model for reading Captchas](/examples/vision/captcha_ocr)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Handwriting recognition](/examples/vision/handwriting_recognition)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 이미지 강화

V3
{: .label .label-green .mx-1}
[Convolutional autoencoder for image denoising](/examples/vision/autoencoder)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Low-light image enhancement using MIRNet](/examples/vision/mirnet)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image Super-Resolution using an Efficient Sub-Pixel CNN](/examples/vision/super_resolution_sub_pixel)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Enhanced Deep Residual Networks for single-image super-resolution](/examples/vision/edsr)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Zero-DCE for low-light image enhancement](/examples/vision/zero_dce)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 데이터 보강

V3
{: .label .label-green .mx-1}
[CutMix data augmentation for image classification](/examples/vision/cutmix)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[MixUp augmentation for image classification](/examples/vision/mixup)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[RandAugment for Image Classification for Improved Robustness](/examples/vision/randaugment)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 이미지 & 텍스트

★
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[Image captioning](/examples/vision/image_captioning)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Natural language image search with a Dual Encoder](/examples/vision/nl_image_search)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 비전 모델 해석 가능성(interpretability)

V3
{: .label .label-green .mx-1}
[Visualizing what convnets learn](/examples/vision/visualizing_what_convnets_learn)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Model interpretability with Integrated Gradients](/examples/vision/integrated_gradients)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Investigating Vision Transformer representations](/examples/vision/probing_vits)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Grad-CAM class activation visualization](/examples/vision/grad_cam)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 이미지 유사도 검색

V2
{: .label .label-yellow .mx-1}
[Near-duplicate image search](/examples/vision/near_dup_search)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Semantic Image Clustering](/examples/vision/semantic_image_clustering)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image similarity estimation using a Siamese Network with a contrastive loss](/examples/vision/siamese_contrastive)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Image similarity estimation using a Siamese Network with a triplet loss](/examples/vision/siamese_network)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Metric learning for image similarity search](/examples/vision/metric_learning)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Metric learning for image similarity search using TensorFlow Similarity](/examples/vision/metric_learning_tf_similarity)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Self-supervised contrastive learning with NNCLR](/examples/vision/nnclr)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 비디오

V3
{: .label .label-green .mx-1}
[Video Classification with a CNN-RNN Architecture](/examples/vision/video_classification)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Next-Frame Video Prediction with Convolutional LSTMs](/examples/vision/conv_lstm)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Video Classification with Transformers](/examples/vision/video_transformers)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Video Vision Transformer](/examples/vision/vivit)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 성능 레시피

V3
{: .label .label-green .mx-1}
[Gradient Centralization for Better Training Performance](/examples/vision/gradient_centralization)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Learning to tokenize in Vision Transformers](/examples/vision/token_learner)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Knowledge Distillation](/examples/vision/knowledge_distillation)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[FixRes: Fixing train-test resolution discrepancy](/examples/vision/fixres)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Class Attention Image Transformers with LayerScale](/examples/vision/cait)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Augmenting convnets with aggregated attention](/examples/vision/patch_convnet)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Learning to Resize](/examples/vision/learnable_resizer)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 기타

V2
{: .label .label-yellow .mx-1}
[Semi-supervision and domain adaptation with AdaMatch](/examples/vision/adamatch)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Barlow Twins for Contrastive SSL](/examples/vision/barlow_twins)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Consistency training with supervision](/examples/vision/consistency_training)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Distilling Vision Transformers](/examples/vision/deit)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Focal Modulation: A replacement for Self-Attention](/examples/vision/focal_modulation_network)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Using the Forward-Forward Algorithm for Image Classification](/examples/vision/forwardforward)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Masked image modeling with Autoencoders](/examples/vision/masked_image_modeling)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Segment Anything Model with 🤗Transformers](/examples/vision/sam)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Semantic segmentation with SegFormer and Hugging Face Transformers](/examples/vision/segformer)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Self-supervised contrastive learning with SimSiam](/examples/vision/simsiam)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Supervised Contrastive Learning](/examples/vision/supervised-contrastive-learning)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[When Recurrence meets Transformers](/examples/vision/temporal_latent_bottleneck)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Efficient Object Detection with YOLOV8 and KerasCV](/examples/vision/yolov8)
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

* * *
