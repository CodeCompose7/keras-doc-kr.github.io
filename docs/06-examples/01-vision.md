---
layout: default
title: 컴퓨터 비전
nav_order: 1
permalink: /examples/vision/
parent: 코드 예제
has_children: true
---

* 원본 링크 : [https://keras.io/examples/vision/](https://keras.io/examples/vision/){:target="_blank"}
* 최종 수정일 : 2024-04-01

# 컴퓨터 비전 (Computer Vision)
{: .no_toc }

## 목차
{: .no_toc .text-delta }

1. TOC
{:toc}

---

### 이미지 분류

★
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[처음부터 이미지 분류 (Image classification from scratch)]({% link docs/06-examples/01-vision/01-image_classification_from_scratch.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

★
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[간단한 MNIST convnet (Simple MNIST convnet)]({% link docs/06-examples/01-vision/02-mnist_convnet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

★
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[EfficientNet으로 하는 미세 조정을 통한 이미지 분류 (Image classification via fine-tuning with EfficientNet)]({% link docs/06-examples/01-vision/03-image_classification_efficientnet_fine_tuning.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[비전 트랜스포머로 이미지 분류 (Image classification with Vision Transformer)]({% link docs/06-examples/01-vision/04-image_classification_with_vision_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[어텐션 기반 심층 다중 인스턴스 학습(MIL)을 사용한 분류 (Classification using Attention-based Deep Multiple Instance Learning)]({% link docs/06-examples/01-vision/05-attention_mil_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[최신 MLP 모델을 사용한 이미지 분류 (Image classification with modern MLP models)]({% link docs/06-examples/01-vision/06-mlp_image_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[이미지 분류를 위한 모바일 친화적인 트랜스포머 기반 모델 (A mobile-friendly Transformer-based model for image classification)]({% link docs/06-examples/01-vision/07-mobilevit.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[TPU에서 폐렴 분류 (Pneumonia Classification on TPU)]({% link docs/06-examples/01-vision/08-xray_classification_with_tpus.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[컴팩트 컨볼루션 트랜스포머 (Compact Convolutional Transformers)]({% link docs/06-examples/01-vision/09-cct.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ConvMixer로 이미지 분류 (Image classification with ConvMixer)]({% link docs/06-examples/01-vision/10-convmixer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[EANet(External Attention Transformer)을 사용한 이미지 분류 (Image classification with EANet (External Attention Transformer))]({% link docs/06-examples/01-vision/11-eanet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[인볼루션 신경망 (Involutional neural networks)]({% link docs/06-examples/01-vision/12-involution.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Perceiver로 이미지 분류 (Image classification with Perceiver)]({% link docs/06-examples/01-vision/13-perceiver_image_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Reptile을 사용한 퓨샷 학습 (Few-Shot learning with Reptile)]({% link docs/06-examples/01-vision/14-reptile.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[SimCLR을 사용한 대조 사전 트레이닝을 사용한 반지도 이미지 분류 (Semi-supervised image classification using contrastive pretraining with SimCLR)]({% link docs/06-examples/01-vision/15-semisupervised_simclr.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Swin 트랜스포머를 사용한 이미지 분류 (Image classification with Swin Transformers)]({% link docs/06-examples/01-vision/16-swin_transformers.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[소규모 데이터 세트에 대해 비전 트랜스포머 트레이닝 (Train a Vision Transformer on small datasets)]({% link docs/06-examples/01-vision/17-vit_small_ds.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[어텐션이 없는 비전 트랜스포머 (A Vision Transformer without Attention)]({% link docs/06-examples/01-vision/18-shiftvit.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[글로벌 컨텍스트 비전 트랜스포머를 이용한 이미지 분류 (Image Classification using Global Context Vision Transformer)]({% link docs/06-examples/01-vision/19-image_classification_using_global_context_vision_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[BigTransfer(BiT)를 사용한 이미지 분류 (Image Classification using BigTransfer (BiT))]({% link docs/06-examples/01-vision/20-bit.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 이미지 세그멘테이션

★
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[U-Net과 유사한 아키텍처를 사용한 이미지 세그멘테이션 (Image segmentation with a U-Net-like architecture)]({% link docs/06-examples/01-vision/21-oxford_pets_image_segmentation.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[DeepLabV3+를 사용한 다중 클래스 시맨틱 세그멘테이션 (Multiclass semantic segmentation using DeepLabV3+)]({% link docs/06-examples/01-vision/22-deeplabv3_plus.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[BASNet을 사용한 매우 정확한 경계 세그멘테이션 (Highly accurate boundaries segmentation using BASNet)]({% link docs/06-examples/01-vision/23-basnet_segmentation.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Composable 완전 컨볼루션 네트워크를 사용한 이미지 세그멘테이션 (Image Segmentation using Composable Fully-Convolutional Networks)]({% link docs/06-examples/01-vision/24-fully_convolutional_network.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 객체 감지

V2
{: .label .label-yellow .mx-1}
[RetinaNet을 이용한 객체 감지 (Object Detection with RetinaNet)]({% link docs/06-examples/01-vision/25-retinanet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[전이 학습을 통한 키포인트 감지 (Keypoint Detection with Transfer Learning)]({% link docs/06-examples/01-vision/26-keypoint_detection.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[비전 트랜스포머를 사용한 객체 감지 (Object detection with Vision Transformers)]({% link docs/06-examples/01-vision/27-object_detection_using_vision_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 3D

V3
{: .label .label-green .mx-1}
[CT 스캔의 3D 이미지 분류 (3D image classification from CT scans)]({% link docs/06-examples/01-vision/28-3D_image_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[단안 깊이 추정 (Monocular depth estimation)]({% link docs/06-examples/01-vision/29-depth_estimation.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

★
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[NeRF를 사용한 3D 체적 렌더링 (3D volumetric rendering with NeRF)]({% link docs/06-examples/01-vision/30-nerf.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[PointNet을 사용한 포인트 클라우드 세그멘테이션 (Point cloud segmentation with PointNet)]({% link docs/06-examples/01-vision/31-pointnet_segmentation.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[PointNet을 사용한 포인트 클라우드 분류 (Point cloud classification)]({% link docs/06-examples/01-vision/32-pointnet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### OCR

V3
{: .label .label-green .mx-1}
[캡챠 읽기를 위한 OCR 모델 (OCR model for reading Captchas)]({% link docs/06-examples/01-vision/33-captcha_ocr.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[손글씨 인식 (Handwriting recognition)]({% link docs/06-examples/01-vision/34-handwriting_recognition.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 이미지 강화

V3
{: .label .label-green .mx-1}
[이미지 노이즈 제거를 위한 컨볼루셔널 오토인코더 (Convolutional autoencoder for image denoising)]({% link docs/06-examples/01-vision/35-autoencoder.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[MIRNet을 사용한 저조도 이미지 향상 (Low-light image enhancement using MIRNet)]({% link docs/06-examples/01-vision/36-mirnet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Efficient Sub-Pixel CNN을 사용한 이미지 초해상도 (Image Super-Resolution using an Efficient Sub-Pixel CNN)]({% link docs/06-examples/01-vision/37-super_resolution_sub_pixel.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[단일 이미지 초해상도를 위한 향상된 깊은 Residual 네트워크 (Enhanced Deep Residual Networks for single-image super-resolution)]({% link docs/06-examples/01-vision/38-edsr.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[저조도 이미지 향상을 위한 Zero-DCE (Zero-DCE for low-light image enhancement)]({% link docs/06-examples/01-vision/39-zero_dce.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 데이터 보강

V3
{: .label .label-green .mx-1}
[이미지 분류를 위한 CutMix 데이터 보강 (CutMix data augmentation for image classification)]({% link docs/06-examples/01-vision/40-cutmix.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[이미지 분류를 위한 MixUp 보강 (MixUp augmentation for image classification)]({% link docs/06-examples/01-vision/41-mixup.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[견고성 향상을 위한 이미지 분류를 위한 RandAugment (RandAugment for Image Classification for Improved Robustness)]({% link docs/06-examples/01-vision/42-randaugment.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 이미지 & 텍스트

★
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[이미지 캡션 (Image captioning)]({% link docs/06-examples/01-vision/43-image_captioning.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[듀얼 인코더를 이용한 자연어 이미지 검색 (Natural language image search with a Dual Encoder)]({% link docs/06-examples/01-vision/44-nl_image_search.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 비전 모델 해석 가능성(interpretability)

V3
{: .label .label-green .mx-1}
[Convnets이 학습한 내용 시각화 (Visualizing what convnets learn)]({% link docs/06-examples/01-vision/45-visualizing_what_convnets_learn.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[통합 그래디언트를 통한 모델 해석 가능성 (Model interpretability with Integrated Gradients)]({% link docs/06-examples/01-vision/46-integrated_gradients.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[비전 트랜스포머 표현 조사 (Investigating Vision Transformer representations)]({% link docs/06-examples/01-vision/47-probing_vits.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Grad-CAM 클래스 활성화 시각화 (Grad-CAM class activation visualization)]({% link docs/06-examples/01-vision/48-grad_cam.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 이미지 유사도 검색

V2
{: .label .label-yellow .mx-1}
[중복에 가까운 이미지 검색 (Near-duplicate image search)]({% link docs/06-examples/01-vision/49-near_dup_search.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[시맨틱 이미지 클러스터링 (Semantic Image Clustering)]({% link docs/06-examples/01-vision/50-semantic_image_clustering.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[대비 손실이 있는 Siamese 네트워크를 사용한 이미지 유사도 추정 (Image similarity estimation using a Siamese Network with a contrastive loss)]({% link docs/06-examples/01-vision/51-siamese_contrastive.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[삼중(triplet) 손실이 있는 Siamese 네트워크를 사용한 이미지 유사도 추정 (Image similarity estimation using a Siamese Network with a triplet loss)]({% link docs/06-examples/01-vision/52-siamese_network.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[이미지 유사도 검색을 위한 메트릭 학습 (Metric learning for image similarity search)]({% link docs/06-examples/01-vision/53-metric_learning.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[TensorFlow Similarity를 사용한 이미지 유사도 검색을 위한 메트릭 학습 (Metric learning for image similarity search using TensorFlow Similarity)]({% link docs/06-examples/01-vision/54-metric_learning_tf_similarity.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[NNCLR을 사용한 자기 지도 대조 학습 (Self-supervised contrastive learning with NNCLR)]({% link docs/06-examples/01-vision/55-nnclr.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 비디오

V3
{: .label .label-green .mx-1}
[CNN-RNN 아키텍처를 사용한 비디오 분류 (Video Classification with a CNN-RNN Architecture)]({% link docs/06-examples/01-vision/56-video_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[컨볼루션 LSTM을 사용한 다음 프레임 비디오 예측 (Next-Frame Video Prediction with Convolutional LSTMs)]({% link docs/06-examples/01-vision/57-conv_lstm.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[트랜스포머를 사용한 비디오 분류 (Video Classification with Transformers)]({% link docs/06-examples/01-vision/58-video_transformers.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[비디오 비전 트랜스포머 (Video Vision Transformer)]({% link docs/06-examples/01-vision/59-vivit.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 성능 레시피

V3
{: .label .label-green .mx-1}
[트레이닝 성능 향상을 위한 그래디언트 중앙화 (Gradient Centralization for Better Training Performance)]({% link docs/06-examples/01-vision/60-gradient_centralization.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[비전 트랜스포머에서 토큰화 학습하기 (Learning to tokenize in Vision Transformers)]({% link docs/06-examples/01-vision/61-token_learner.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[지식 증류 (Knowledge Distillation)]({% link docs/06-examples/01-vision/62-knowledge_distillation.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[FixRes: 트레이닝-테스트 해상도 불일치 수정 (FixRes: Fixing train-test resolution discrepancy)]({% link docs/06-examples/01-vision/63-fixres.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[LayerScale을 사용한 클래스 어텐션 이미지 트랜스포머 (Class Attention Image Transformers with LayerScale)]({% link docs/06-examples/01-vision/64-cait.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[통합 어텐션으로 컨볼루션 네트워크 강화 (Augmenting convnets with aggregated attention)]({% link docs/06-examples/01-vision/65-patch_convnet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[컴퓨터 비전에서 리사이즈 학습 (Learning to Resize)]({% link docs/06-examples/01-vision/66-learnable_resizer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 기타

V2
{: .label .label-yellow .mx-1}
[AdaMatch를 통한 반지도 및 도메인 적응 (Semi-supervision and domain adaptation with AdaMatch)]({% link docs/06-examples/01-vision/67-adamatch.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Contrastive SSL을 위한 Barlow Twins (Barlow Twins for Contrastive SSL)]({% link docs/06-examples/01-vision/68-barlow_twins.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[지도를 통한 일관성 트레이닝 (Consistency training with supervision)]({% link docs/06-examples/01-vision/69-consistency_training.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[증류식 비전 트랜스포머 (Distilling Vision Transformers)]({% link docs/06-examples/01-vision/70-deit.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[초점 변조(Focal Modulation): 셀프 어텐션을 대체하는 (Focal Modulation: A replacement for Self-Attention)]({% link docs/06-examples/01-vision/71-focal_modulation_network.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[이미지 분류를 위한 Forward-Forward 알고리즘 사용 (Using the Forward-Forward Algorithm for Image Classification)]({% link docs/06-examples/01-vision/72-forwardforward.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[자동 인코더를 사용한 마스크 이미지 모델링 (Masked image modeling with Autoencoders)]({% link docs/06-examples/01-vision/73-masked_image_modeling.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[🤗 트랜스포머로 무엇이든 모델 세그먼트 (Segment Anything Model with 🤗Transformers)]({% link docs/06-examples/01-vision/74-sam.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[SegFormer와 Hugging Face 트랜스포머를 사용한 시맨틱 세그멘테이션 (Semantic segmentation with SegFormer and Hugging Face Transformers)]({% link docs/06-examples/01-vision/75-segformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[SimSiam을 이용한 자기 지도 대조 학습 (Self-supervised contrastive learning with SimSiam)]({% link docs/06-examples/01-vision/76-simsiam.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[지도 대조 학습 (Supervised Contrastive Learning)]({% link docs/06-examples/01-vision/77-supervised-contrastive-learning.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Recurrence와 트랜스포머의 만남 (When Recurrence meets Transformers)]({% link docs/06-examples/01-vision/78-temporal_latent_bottleneck.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[YOLOV8 및 KerasCV를 통한 효율적인 객체 감지 (Efficient Object Detection with YOLOV8 and KerasCV)]({% link docs/06-examples/01-vision/79-yolov8.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}
