---
layout: default
title: ì»´í“¨í„° ë¹„ì „
nav_order: 1
permalink: /examples/vision/
parent: ì½”ë“œ ì˜ˆì œ
has_children: true
---

* ì›ë³¸ ë§í¬ : [https://keras.io/examples/vision/](https://keras.io/examples/vision/){:target="_blank"}
* ìµœì¢… ìˆ˜ì •ì¼ : 2024-04-01

# ì»´í“¨í„° ë¹„ì „ (Computer Vision)
{: .no_toc }

## ëª©ì°¨
{: .no_toc .text-delta }

1. TOC
{:toc}

---

### ì´ë¯¸ì§€ ë¶„ë¥˜

â˜…
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[ì²˜ìŒë¶€í„° ì´ë¯¸ì§€ ë¶„ë¥˜ (Image classification from scratch)]({% link docs/06-examples/01-vision/01-image_classification_from_scratch.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

â˜…
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[ê°„ë‹¨í•œ MNIST convnet (Simple MNIST convnet)]({% link docs/06-examples/01-vision/02-mnist_convnet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

â˜…
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[EfficientNetìœ¼ë¡œ í•˜ëŠ” ë¯¸ì„¸ ì¡°ì •ì„ í†µí•œ ì´ë¯¸ì§€ ë¶„ë¥˜ (Image classification via fine-tuning with EfficientNet)]({% link docs/06-examples/01-vision/03-image_classification_efficientnet_fine_tuning.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ ì´ë¯¸ì§€ ë¶„ë¥˜ (Image classification with Vision Transformer)]({% link docs/06-examples/01-vision/04-image_classification_with_vision_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì–´í…ì…˜ ê¸°ë°˜ ì‹¬ì¸µ ë‹¤ì¤‘ ì¸ìŠ¤í„´ìŠ¤ í•™ìŠµ(MIL)ì„ ì‚¬ìš©í•œ ë¶„ë¥˜ (Classification using Attention-based Deep Multiple Instance Learning)]({% link docs/06-examples/01-vision/05-attention_mil_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ìµœì‹  MLP ëª¨ë¸ì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ (Image classification with modern MLP models)]({% link docs/06-examples/01-vision/06-mlp_image_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìœ„í•œ ëª¨ë°”ì¼ ì¹œí™”ì ì¸ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ (A mobile-friendly Transformer-based model for image classification)]({% link docs/06-examples/01-vision/07-mobilevit.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[TPUì—ì„œ íë ´ ë¶„ë¥˜ (Pneumonia Classification on TPU)]({% link docs/06-examples/01-vision/08-xray_classification_with_tpus.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì»´íŒ©íŠ¸ ì»¨ë³¼ë£¨ì…˜ íŠ¸ëœìŠ¤í¬ë¨¸ (Compact Convolutional Transformers)]({% link docs/06-examples/01-vision/09-cct.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ConvMixerë¡œ ì´ë¯¸ì§€ ë¶„ë¥˜ (Image classification with ConvMixer)]({% link docs/06-examples/01-vision/10-convmixer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[EANet(External Attention Transformer)ì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ (Image classification with EANet (External Attention Transformer))]({% link docs/06-examples/01-vision/11-eanet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì¸ë³¼ë£¨ì…˜ ì‹ ê²½ë§ (Involutional neural networks)]({% link docs/06-examples/01-vision/12-involution.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Perceiverë¡œ ì´ë¯¸ì§€ ë¶„ë¥˜ (Image classification with Perceiver)]({% link docs/06-examples/01-vision/13-perceiver_image_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Reptileì„ ì‚¬ìš©í•œ í“¨ìƒ· í•™ìŠµ (Few-Shot learning with Reptile)]({% link docs/06-examples/01-vision/14-reptile.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[SimCLRì„ ì‚¬ìš©í•œ ëŒ€ì¡° ì‚¬ì „ íŠ¸ë ˆì´ë‹ì„ ì‚¬ìš©í•œ ë°˜ì§€ë„ ì´ë¯¸ì§€ ë¶„ë¥˜ (Semi-supervised image classification using contrastive pretraining with SimCLR)]({% link docs/06-examples/01-vision/15-semisupervised_simclr.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Swin íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ (Image classification with Swin Transformers)]({% link docs/06-examples/01-vision/16-swin_transformers.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ì†Œê·œëª¨ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•´ ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ íŠ¸ë ˆì´ë‹ (Train a Vision Transformer on small datasets)]({% link docs/06-examples/01-vision/17-vit_small_ds.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ì–´í…ì…˜ì´ ì—†ëŠ” ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ (A Vision Transformer without Attention)]({% link docs/06-examples/01-vision/18-shiftvit.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ê¸€ë¡œë²Œ ì»¨í…ìŠ¤íŠ¸ ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì´ìš©í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ (Image Classification using Global Context Vision Transformer)]({% link docs/06-examples/01-vision/19-image_classification_using_global_context_vision_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[BigTransfer(BiT)ë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ (Image Classification using BigTransfer (BiT))]({% link docs/06-examples/01-vision/20-bit.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜

â˜…
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[U-Netê³¼ ìœ ì‚¬í•œ ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ (Image segmentation with a U-Net-like architecture)]({% link docs/06-examples/01-vision/21-oxford_pets_image_segmentation.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[DeepLabV3+ë¥¼ ì‚¬ìš©í•œ ë‹¤ì¤‘ í´ë˜ìŠ¤ ì‹œë§¨í‹± ì„¸ê·¸ë©˜í…Œì´ì…˜ (Multiclass semantic segmentation using DeepLabV3+)]({% link docs/06-examples/01-vision/22-deeplabv3_plus.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[BASNetì„ ì‚¬ìš©í•œ ë§¤ìš° ì •í™•í•œ ê²½ê³„ ì„¸ê·¸ë©˜í…Œì´ì…˜ (Highly accurate boundaries segmentation using BASNet)]({% link docs/06-examples/01-vision/23-basnet_segmentation.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Composable ì™„ì „ ì»¨ë³¼ë£¨ì…˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ (Image Segmentation using Composable Fully-Convolutional Networks)]({% link docs/06-examples/01-vision/24-fully_convolutional_network.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ê°ì²´ ê°ì§€

V2
{: .label .label-yellow .mx-1}
[RetinaNetì„ ì´ìš©í•œ ê°ì²´ ê°ì§€ (Object Detection with RetinaNet)]({% link docs/06-examples/01-vision/25-retinanet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì „ì´ í•™ìŠµì„ í†µí•œ í‚¤í¬ì¸íŠ¸ ê°ì§€ (Keypoint Detection with Transfer Learning)]({% link docs/06-examples/01-vision/26-keypoint_detection.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•œ ê°ì²´ ê°ì§€ (Object detection with Vision Transformers)]({% link docs/06-examples/01-vision/27-object_detection_using_vision_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 3D

V3
{: .label .label-green .mx-1}
[CT ìŠ¤ìº”ì˜ 3D ì´ë¯¸ì§€ ë¶„ë¥˜ (3D image classification from CT scans)]({% link docs/06-examples/01-vision/28-3D_image_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ë‹¨ì•ˆ ê¹Šì´ ì¶”ì • (Monocular depth estimation)]({% link docs/06-examples/01-vision/29-depth_estimation.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

â˜…
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[NeRFë¥¼ ì‚¬ìš©í•œ 3D ì²´ì  ë Œë”ë§ (3D volumetric rendering with NeRF)]({% link docs/06-examples/01-vision/30-nerf.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[PointNetì„ ì‚¬ìš©í•œ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ì„¸ê·¸ë©˜í…Œì´ì…˜ (Point cloud segmentation with PointNet)]({% link docs/06-examples/01-vision/31-pointnet_segmentation.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[PointNetì„ ì‚¬ìš©í•œ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë¶„ë¥˜ (Point cloud classification)]({% link docs/06-examples/01-vision/32-pointnet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### OCR

V3
{: .label .label-green .mx-1}
[ìº¡ì±  ì½ê¸°ë¥¼ ìœ„í•œ OCR ëª¨ë¸ (OCR model for reading Captchas)]({% link docs/06-examples/01-vision/33-captcha_ocr.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ì†ê¸€ì”¨ ì¸ì‹ (Handwriting recognition)]({% link docs/06-examples/01-vision/34-handwriting_recognition.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ì´ë¯¸ì§€ ê°•í™”

V3
{: .label .label-green .mx-1}
[ì´ë¯¸ì§€ ë…¸ì´ì¦ˆ ì œê±°ë¥¼ ìœ„í•œ ì»¨ë³¼ë£¨ì…”ë„ ì˜¤í† ì¸ì½”ë” (Convolutional autoencoder for image denoising)]({% link docs/06-examples/01-vision/35-autoencoder.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[MIRNetì„ ì‚¬ìš©í•œ ì €ì¡°ë„ ì´ë¯¸ì§€ í–¥ìƒ (Low-light image enhancement using MIRNet)]({% link docs/06-examples/01-vision/36-mirnet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Efficient Sub-Pixel CNNì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ì´ˆí•´ìƒë„ (Image Super-Resolution using an Efficient Sub-Pixel CNN)]({% link docs/06-examples/01-vision/37-super_resolution_sub_pixel.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ë‹¨ì¼ ì´ë¯¸ì§€ ì´ˆí•´ìƒë„ë¥¼ ìœ„í•œ í–¥ìƒëœ ê¹Šì€ Residual ë„¤íŠ¸ì›Œí¬ (Enhanced Deep Residual Networks for single-image super-resolution)]({% link docs/06-examples/01-vision/38-edsr.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì €ì¡°ë„ ì´ë¯¸ì§€ í–¥ìƒì„ ìœ„í•œ Zero-DCE (Zero-DCE for low-light image enhancement)]({% link docs/06-examples/01-vision/39-zero_dce.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ë°ì´í„° ë³´ê°•

V3
{: .label .label-green .mx-1}
[ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìœ„í•œ CutMix ë°ì´í„° ë³´ê°• (CutMix data augmentation for image classification)]({% link docs/06-examples/01-vision/40-cutmix.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìœ„í•œ MixUp ë³´ê°• (MixUp augmentation for image classification)]({% link docs/06-examples/01-vision/41-mixup.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ê²¬ê³ ì„± í–¥ìƒì„ ìœ„í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìœ„í•œ RandAugment (RandAugment for Image Classification for Improved Robustness)]({% link docs/06-examples/01-vision/42-randaugment.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ì´ë¯¸ì§€ & í…ìŠ¤íŠ¸

â˜…
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[ì´ë¯¸ì§€ ìº¡ì…˜ (Image captioning)]({% link docs/06-examples/01-vision/43-image_captioning.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ë“€ì–¼ ì¸ì½”ë”ë¥¼ ì´ìš©í•œ ìì—°ì–´ ì´ë¯¸ì§€ ê²€ìƒ‰ (Natural language image search with a Dual Encoder)]({% link docs/06-examples/01-vision/44-nl_image_search.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ë¹„ì „ ëª¨ë¸ í•´ì„ ê°€ëŠ¥ì„±(interpretability)

V3
{: .label .label-green .mx-1}
[Convnetsì´ í•™ìŠµí•œ ë‚´ìš© ì‹œê°í™” (Visualizing what convnets learn)]({% link docs/06-examples/01-vision/45-visualizing_what_convnets_learn.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[í†µí•© ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ í†µí•œ ëª¨ë¸ í•´ì„ ê°€ëŠ¥ì„± (Model interpretability with Integrated Gradients)]({% link docs/06-examples/01-vision/46-integrated_gradients.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ í‘œí˜„ ì¡°ì‚¬ (Investigating Vision Transformer representations)]({% link docs/06-examples/01-vision/47-probing_vits.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Grad-CAM í´ë˜ìŠ¤ í™œì„±í™” ì‹œê°í™” (Grad-CAM class activation visualization)]({% link docs/06-examples/01-vision/48-grad_cam.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê²€ìƒ‰

V2
{: .label .label-yellow .mx-1}
[ì¤‘ë³µì— ê°€ê¹Œìš´ ì´ë¯¸ì§€ ê²€ìƒ‰ (Near-duplicate image search)]({% link docs/06-examples/01-vision/49-near_dup_search.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì‹œë§¨í‹± ì´ë¯¸ì§€ í´ëŸ¬ìŠ¤í„°ë§ (Semantic Image Clustering)]({% link docs/06-examples/01-vision/50-semantic_image_clustering.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ëŒ€ë¹„ ì†ì‹¤ì´ ìˆëŠ” Siamese ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ìœ ì‚¬ë„ ì¶”ì • (Image similarity estimation using a Siamese Network with a contrastive loss)]({% link docs/06-examples/01-vision/51-siamese_contrastive.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì‚¼ì¤‘(triplet) ì†ì‹¤ì´ ìˆëŠ” Siamese ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ìœ ì‚¬ë„ ì¶”ì • (Image similarity estimation using a Siamese Network with a triplet loss)]({% link docs/06-examples/01-vision/52-siamese_network.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìœ„í•œ ë©”íŠ¸ë¦­ í•™ìŠµ (Metric learning for image similarity search)]({% link docs/06-examples/01-vision/53-metric_learning.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[TensorFlow Similarityë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìœ„í•œ ë©”íŠ¸ë¦­ í•™ìŠµ (Metric learning for image similarity search using TensorFlow Similarity)]({% link docs/06-examples/01-vision/54-metric_learning_tf_similarity.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[NNCLRì„ ì‚¬ìš©í•œ ìê¸° ì§€ë„ ëŒ€ì¡° í•™ìŠµ (Self-supervised contrastive learning with NNCLR)]({% link docs/06-examples/01-vision/55-nnclr.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ë¹„ë””ì˜¤

V3
{: .label .label-green .mx-1}
[CNN-RNN ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•œ ë¹„ë””ì˜¤ ë¶„ë¥˜ (Video Classification with a CNN-RNN Architecture)]({% link docs/06-examples/01-vision/56-video_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì»¨ë³¼ë£¨ì…˜ LSTMì„ ì‚¬ìš©í•œ ë‹¤ìŒ í”„ë ˆì„ ë¹„ë””ì˜¤ ì˜ˆì¸¡ (Next-Frame Video Prediction with Convolutional LSTMs)]({% link docs/06-examples/01-vision/57-conv_lstm.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•œ ë¹„ë””ì˜¤ ë¶„ë¥˜ (Video Classification with Transformers)]({% link docs/06-examples/01-vision/58-video_transformers.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ë¹„ë””ì˜¤ ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ (Video Vision Transformer)]({% link docs/06-examples/01-vision/59-vivit.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ì„±ëŠ¥ ë ˆì‹œí”¼

V3
{: .label .label-green .mx-1}
[íŠ¸ë ˆì´ë‹ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ê·¸ë˜ë””ì–¸íŠ¸ ì¤‘ì•™í™” (Gradient Centralization for Better Training Performance)]({% link docs/06-examples/01-vision/60-gradient_centralization.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ í† í°í™” í•™ìŠµí•˜ê¸° (Learning to tokenize in Vision Transformers)]({% link docs/06-examples/01-vision/61-token_learner.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì§€ì‹ ì¦ë¥˜ (Knowledge Distillation)]({% link docs/06-examples/01-vision/62-knowledge_distillation.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[FixRes: íŠ¸ë ˆì´ë‹-í…ŒìŠ¤íŠ¸ í•´ìƒë„ ë¶ˆì¼ì¹˜ ìˆ˜ì • (FixRes: Fixing train-test resolution discrepancy)]({% link docs/06-examples/01-vision/63-fixres.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[LayerScaleì„ ì‚¬ìš©í•œ í´ë˜ìŠ¤ ì–´í…ì…˜ ì´ë¯¸ì§€ íŠ¸ëœìŠ¤í¬ë¨¸ (Class Attention Image Transformers with LayerScale)]({% link docs/06-examples/01-vision/64-cait.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[í†µí•© ì–´í…ì…˜ìœ¼ë¡œ ì»¨ë³¼ë£¨ì…˜ ë„¤íŠ¸ì›Œí¬ ê°•í™” (Augmenting convnets with aggregated attention)]({% link docs/06-examples/01-vision/65-patch_convnet.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì»´í“¨í„° ë¹„ì „ì—ì„œ ë¦¬ì‚¬ì´ì¦ˆ í•™ìŠµ (Learning to Resize)]({% link docs/06-examples/01-vision/66-learnable_resizer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ê¸°íƒ€

V2
{: .label .label-yellow .mx-1}
[AdaMatchë¥¼ í†µí•œ ë°˜ì§€ë„ ë° ë„ë©”ì¸ ì ì‘ (Semi-supervision and domain adaptation with AdaMatch)]({% link docs/06-examples/01-vision/67-adamatch.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Contrastive SSLì„ ìœ„í•œ Barlow Twins (Barlow Twins for Contrastive SSL)]({% link docs/06-examples/01-vision/68-barlow_twins.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ì§€ë„ë¥¼ í†µí•œ ì¼ê´€ì„± íŠ¸ë ˆì´ë‹ (Consistency training with supervision)]({% link docs/06-examples/01-vision/69-consistency_training.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ì¦ë¥˜ì‹ ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ (Distilling Vision Transformers)]({% link docs/06-examples/01-vision/70-deit.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ì´ˆì  ë³€ì¡°(Focal Modulation): ì…€í”„ ì–´í…ì…˜ì„ ëŒ€ì²´í•˜ëŠ” (Focal Modulation: A replacement for Self-Attention)]({% link docs/06-examples/01-vision/71-focal_modulation_network.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìœ„í•œ Forward-Forward ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš© (Using the Forward-Forward Algorithm for Image Classification)]({% link docs/06-examples/01-vision/72-forwardforward.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ìë™ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•œ ë§ˆìŠ¤í¬ ì´ë¯¸ì§€ ëª¨ë¸ë§ (Masked image modeling with Autoencoders)]({% link docs/06-examples/01-vision/73-masked_image_modeling.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ğŸ¤— íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ ë¬´ì—‡ì´ë“  ëª¨ë¸ ì„¸ê·¸ë¨¼íŠ¸ (Segment Anything Model with ğŸ¤—Transformers)]({% link docs/06-examples/01-vision/74-sam.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[SegFormerì™€ Hugging Face íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•œ ì‹œë§¨í‹± ì„¸ê·¸ë©˜í…Œì´ì…˜ (Semantic segmentation with SegFormer and Hugging Face Transformers)]({% link docs/06-examples/01-vision/75-segformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[SimSiamì„ ì´ìš©í•œ ìê¸° ì§€ë„ ëŒ€ì¡° í•™ìŠµ (Self-supervised contrastive learning with SimSiam)]({% link docs/06-examples/01-vision/76-simsiam.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ì§€ë„ ëŒ€ì¡° í•™ìŠµ (Supervised Contrastive Learning)]({% link docs/06-examples/01-vision/77-supervised-contrastive-learning.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Recurrenceì™€ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ë§Œë‚¨ (When Recurrence meets Transformers)]({% link docs/06-examples/01-vision/78-temporal_latent_bottleneck.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[YOLOV8 ë° KerasCVë¥¼ í†µí•œ íš¨ìœ¨ì ì¸ ê°ì²´ ê°ì§€ (Efficient Object Detection with YOLOV8 and KerasCV)]({% link docs/06-examples/01-vision/79-yolov8.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}
