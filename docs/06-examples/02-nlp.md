---
layout: default
title: 자연어 처리
nav_order: 2
permalink: /examples/nlp/
parent: 코드 예제
has_children: true
---

* 원본 링크 : [https://keras.io/examples/nlp/](https://keras.io/examples/nlp/){:target="_blank"}
* 최종 수정일 : 2024-04-01

# 자연어 처리 (Natural Language Processing)
{: .no_toc }

## 목차
{: .no_toc .text-delta }

1. TOC
{:toc}

---

### 텍스트 분류
{: #text-classification}
<!-- ### Text classification -->

★
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[처음부터 텍스트 분류 (Text classification from scratch)]({% link docs/06-examples/02-nlp/01-text_classification_from_scratch.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Active 학습을 사용한 분류 리뷰 (Review Classification using Active Learning)]({% link docs/06-examples/02-nlp/02-active_learning_review_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[FNet을 사용한 텍스트 분류 (Text Classification using FNet)]({% link docs/06-examples/02-nlp/03-fnet_classification_with_keras_nlp.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[대규모 다중 레이블 텍스트 분류 (Large-scale multi-label text classification)]({% link docs/06-examples/02-nlp/04-multi_label_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[트랜스포머로 텍스트 분류 (Text classification with Transformer)]({% link docs/06-examples/02-nlp/05-text_classification_with_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[스위치 트랜스포머로 텍스트 분류 (Text classification with Switch Transformer)]({% link docs/06-examples/02-nlp/06-text_classification_with_switch_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[의사 결정 포레스트와 사전 트레이닝된 임베딩을 사용한 텍스트 분류 (Text classification using Decision Forests and pretrained embeddings)]({% link docs/06-examples/02-nlp/07-tweet-classification-using-tfdf.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[사전 트레이닝된 단어 임베딩 사용 (Using pre-trained word embeddings)]({% link docs/06-examples/02-nlp/08-pretrained_word_embeddings.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[IMDB에 대한 양방향 LSTM (Bidirectional LSTM on IMDB)]({% link docs/06-examples/02-nlp/09-bidirectional_lstm_imdb.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[KerasNLP 및 tf.distribute를 사용한 데이터 병렬 트레이닝 (Data Parallel Training with KerasNLP and tf.distribute)]({% link docs/06-examples/02-nlp/10-data_parallel_training_with_keras_nlp.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 기계 번역
{: #machine-translation}
<!-- ### Machine translation -->

V3
{: .label .label-green .mx-1}
[KerasNLP를 사용한 영어-스페인어 번역 (English-to-Spanish translation with KerasNLP)]({% link docs/06-examples/02-nlp/11-neural_machine_translation_with_keras_nlp.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

★
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[시퀀스-to-시퀀스 트랜스포머를 사용한 영어-스페인어 번역 (English-to-Spanish translation with a sequence-to-sequence Transformer)]({% link docs/06-examples/02-nlp/12-neural_machine_translation_with_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[문자 레벨 recurrent 시퀀스-to-시퀀스 모델 (Character-level recurrent sequence-to-sequence model)]({% link docs/06-examples/02-nlp/13-lstm_seq2seq.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 연관성 예측 (Entailment prediction)
{: #entailment-prediction}
<!-- ### Entailment prediction -->

V2
{: .label .label-yellow .mx-1}
[멀티모달 수반 (Multimodal entailment)]({% link docs/06-examples/02-nlp/14-multimodal_entailment.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 명명된 엔티티 인식
{: #named-entity-recognition}
<!-- ### Named entity recognition -->

V3
{: .label .label-green .mx-1}
[트랜스포머를 사용한 명명된 엔티티 인식 (Named Entity Recognition using Transformers)]({% link docs/06-examples/02-nlp/15-ner_transformers.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Sequence-to-sequence
{: #sequence-to-sequence}

V2
{: .label .label-yellow .mx-1}
[BERT를 사용한 텍스트 추출 (Text Extraction with BERT)]({% link docs/06-examples/02-nlp/16-text_extraction_with_bert.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[숫자 덧셈을 수행하기 위한 시퀀스-to-시퀀스 학습 (Sequence to sequence learning for performing number addition)]({% link docs/06-examples/02-nlp/17-addition_rnn.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 텍스트 유사도 검색
{: #text-similarity-search}
<!-- ### Text similarity search -->

V3
{: .label .label-green .mx-1}
[KerasNLP를 사용한 시맨틱 유사성 (Semantic Similarity with KerasNLP)]({% link docs/06-examples/02-nlp/18-semantic_similarity_with_keras_nlp.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[BERT를 사용한 시맨틱 유사성 (Semantic Similarity with BERT)]({% link docs/06-examples/02-nlp/19-semantic_similarity_with_bert.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Siamese RoBERTa 네트워크를 사용한 문장 임베딩 (Sentence embeddings using Siamese RoBERTa-networks)]({% link docs/06-examples/02-nlp/20-sentence_embeddings_with_sbert.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 언어 모델링
{: #language-modeling}
<!-- ### Language modeling -->

V2
{: .label .label-yellow .mx-1}
[BERT를 사용한 엔드투엔드 마스크 언어 모델링 (End-to-end Masked Language Modeling with BERT)]({% link docs/06-examples/02-nlp/21-masked_language_modeling.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Hugging Face 트랜스포머로 BERT 사전 트레이닝하기 (Pretraining BERT with Hugging Face Transformers)]({% link docs/06-examples/02-nlp/22-pretraining_BERT.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 효율적인 매개변수 미세 조정
{: #parameter-efficient-fine-tuning}
<!-- ### Parameter efficient fine-tuning -->

V3
{: .label .label-green .mx-1}
[LoRA가 있는 GPT-2의 효율적인 파라미터 미세 조정 (Parameter-efficient fine-tuning of GPT-2 with LoRA)]({% link docs/06-examples/02-nlp/23-parameter_efficient_finetuning_of_gpt2_with_lora.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 기타
{: #other}
<!-- ### Other -->

V2
{: .label .label-yellow .mx-1}
[BART를 사용한 추상적 텍스트 요약 (Abstractive Text Summarization with BART)]({% link docs/06-examples/02-nlp/24-abstractive_summarization_with_bart.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[🤗 트랜스포머 및 TPU를 사용하여 처음부터 언어 모델 트레이닝하기 (Training a language model from scratch with 🤗 Transformers and TPUs)]({% link docs/06-examples/02-nlp/25-mlm_training_tpus.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[전이 학습으로 객관식 과제 (MultipleChoice Task with Transfer Learning)]({% link docs/06-examples/02-nlp/26-multiple_choice_task_with_transfer_learning.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Hugging Face 트랜스포머로 질문 답변하기 (Question Answering with Hugging Face Transformers)]({% link docs/06-examples/02-nlp/27-question_answering.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Hugging Face 트랜스포머를 사용한 추상적 요약 (Abstractive Summarization with Hugging Face Transformers)]({% link docs/06-examples/02-nlp/28-t5_hf_summarization.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}
