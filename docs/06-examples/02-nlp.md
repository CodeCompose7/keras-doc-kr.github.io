---
layout: default
title: ìì—°ì–´ ì²˜ë¦¬
nav_order: 2
permalink: /examples/nlp/
parent: ì½”ë“œ ì˜ˆì œ
has_children: true
---

* ì›ë³¸ ë§í¬ : [https://keras.io/examples/nlp/](https://keras.io/examples/nlp/){:target="_blank"}
* ìµœì¢… ìˆ˜ì •ì¼ : 2024-04-01

# ìì—°ì–´ ì²˜ë¦¬ (Natural Language Processing)
{: .no_toc }

## ëª©ì°¨
{: .no_toc .text-delta }

1. TOC
{:toc}

---

### í…ìŠ¤íŠ¸ ë¶„ë¥˜
{: #text-classification}
<!-- ### Text classification -->

â˜…
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[ì²˜ìŒë¶€í„° í…ìŠ¤íŠ¸ ë¶„ë¥˜ (Text classification from scratch)]({% link docs/06-examples/02-nlp/01-text_classification_from_scratch.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Active í•™ìŠµì„ ì‚¬ìš©í•œ ë¶„ë¥˜ ë¦¬ë·° (Review Classification using Active Learning)]({% link docs/06-examples/02-nlp/02-active_learning_review_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[FNetì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ (Text Classification using FNet)]({% link docs/06-examples/02-nlp/03-fnet_classification_with_keras_nlp.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ëŒ€ê·œëª¨ ë‹¤ì¤‘ ë ˆì´ë¸” í…ìŠ¤íŠ¸ ë¶„ë¥˜ (Large-scale multi-label text classification)]({% link docs/06-examples/02-nlp/04-multi_label_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ (Text classification with Transformer)]({% link docs/06-examples/02-nlp/05-text_classification_with_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ìŠ¤ìœ„ì¹˜ íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ (Text classification with Switch Transformer)]({% link docs/06-examples/02-nlp/06-text_classification_with_switch_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ì˜ì‚¬ ê²°ì • í¬ë ˆìŠ¤íŠ¸ì™€ ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ ì„ë² ë”©ì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ (Text classification using Decision Forests and pretrained embeddings)]({% link docs/06-examples/02-nlp/07-tweet-classification-using-tfdf.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ ë‹¨ì–´ ì„ë² ë”© ì‚¬ìš© (Using pre-trained word embeddings)]({% link docs/06-examples/02-nlp/08-pretrained_word_embeddings.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[IMDBì— ëŒ€í•œ ì–‘ë°©í–¥ LSTM (Bidirectional LSTM on IMDB)]({% link docs/06-examples/02-nlp/09-bidirectional_lstm_imdb.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[KerasNLP ë° tf.distributeë¥¼ ì‚¬ìš©í•œ ë°ì´í„° ë³‘ë ¬ íŠ¸ë ˆì´ë‹ (Data Parallel Training with KerasNLP and tf.distribute)]({% link docs/06-examples/02-nlp/10-data_parallel_training_with_keras_nlp.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ê¸°ê³„ ë²ˆì—­
{: #machine-translation}
<!-- ### Machine translation -->

V3
{: .label .label-green .mx-1}
[KerasNLPë¥¼ ì‚¬ìš©í•œ ì˜ì–´-ìŠ¤í˜ì¸ì–´ ë²ˆì—­ (English-to-Spanish translation with KerasNLP)]({% link docs/06-examples/02-nlp/11-neural_machine_translation_with_keras_nlp.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

â˜…
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[ì‹œí€€ìŠ¤-to-ì‹œí€€ìŠ¤ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•œ ì˜ì–´-ìŠ¤í˜ì¸ì–´ ë²ˆì—­ (English-to-Spanish translation with a sequence-to-sequence Transformer)]({% link docs/06-examples/02-nlp/12-neural_machine_translation_with_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ë¬¸ì ë ˆë²¨ recurrent ì‹œí€€ìŠ¤-to-ì‹œí€€ìŠ¤ ëª¨ë¸ (Character-level recurrent sequence-to-sequence model)]({% link docs/06-examples/02-nlp/13-lstm_seq2seq.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ì—°ê´€ì„± ì˜ˆì¸¡ (Entailment prediction)
{: #entailment-prediction}
<!-- ### Entailment prediction -->

V2
{: .label .label-yellow .mx-1}
[ë©€í‹°ëª¨ë‹¬ ìˆ˜ë°˜ (Multimodal entailment)]({% link docs/06-examples/02-nlp/14-multimodal_entailment.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ëª…ëª…ëœ ì—”í‹°í‹° ì¸ì‹
{: #named-entity-recognition}
<!-- ### Named entity recognition -->

V3
{: .label .label-green .mx-1}
[íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•œ ëª…ëª…ëœ ì—”í‹°í‹° ì¸ì‹ (Named Entity Recognition using Transformers)]({% link docs/06-examples/02-nlp/15-ner_transformers.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Sequence-to-sequence
{: #sequence-to-sequence}

V2
{: .label .label-yellow .mx-1}
[BERTë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (Text Extraction with BERT)]({% link docs/06-examples/02-nlp/16-text_extraction_with_bert.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[ìˆ«ì ë§ì…ˆì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ì‹œí€€ìŠ¤-to-ì‹œí€€ìŠ¤ í•™ìŠµ (Sequence to sequence learning for performing number addition)]({% link docs/06-examples/02-nlp/17-addition_rnn.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê²€ìƒ‰
{: #text-similarity-search}
<!-- ### Text similarity search -->

V3
{: .label .label-green .mx-1}
[KerasNLPë¥¼ ì‚¬ìš©í•œ ì‹œë§¨í‹± ìœ ì‚¬ì„± (Semantic Similarity with KerasNLP)]({% link docs/06-examples/02-nlp/18-semantic_similarity_with_keras_nlp.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[BERTë¥¼ ì‚¬ìš©í•œ ì‹œë§¨í‹± ìœ ì‚¬ì„± (Semantic Similarity with BERT)]({% link docs/06-examples/02-nlp/19-semantic_similarity_with_bert.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Siamese RoBERTa ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•œ ë¬¸ì¥ ì„ë² ë”© (Sentence embeddings using Siamese RoBERTa-networks)]({% link docs/06-examples/02-nlp/20-sentence_embeddings_with_sbert.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ì–¸ì–´ ëª¨ë¸ë§
{: #language-modeling}
<!-- ### Language modeling -->

V2
{: .label .label-yellow .mx-1}
[BERTë¥¼ ì‚¬ìš©í•œ ì—”ë“œíˆ¬ì—”ë“œ ë§ˆìŠ¤í¬ ì–¸ì–´ ëª¨ë¸ë§ (End-to-end Masked Language Modeling with BERT)]({% link docs/06-examples/02-nlp/21-masked_language_modeling.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Hugging Face íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ BERT ì‚¬ì „ íŠ¸ë ˆì´ë‹í•˜ê¸° (Pretraining BERT with Hugging Face Transformers)]({% link docs/06-examples/02-nlp/22-pretraining_BERT.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### íš¨ìœ¨ì ì¸ ë§¤ê°œë³€ìˆ˜ ë¯¸ì„¸ ì¡°ì •
{: #parameter-efficient-fine-tuning}
<!-- ### Parameter efficient fine-tuning -->

V3
{: .label .label-green .mx-1}
[LoRAê°€ ìˆëŠ” GPT-2ì˜ íš¨ìœ¨ì ì¸ íŒŒë¼ë¯¸í„° ë¯¸ì„¸ ì¡°ì • (Parameter-efficient fine-tuning of GPT-2 with LoRA)]({% link docs/06-examples/02-nlp/23-parameter_efficient_finetuning_of_gpt2_with_lora.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### ê¸°íƒ€
{: #other}
<!-- ### Other -->

V2
{: .label .label-yellow .mx-1}
[BARTë¥¼ ì‚¬ìš©í•œ ì¶”ìƒì  í…ìŠ¤íŠ¸ ìš”ì•½ (Abstractive Text Summarization with BART)]({% link docs/06-examples/02-nlp/24-abstractive_summarization_with_bart.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ğŸ¤— íŠ¸ëœìŠ¤í¬ë¨¸ ë° TPUë¥¼ ì‚¬ìš©í•˜ì—¬ ì²˜ìŒë¶€í„° ì–¸ì–´ ëª¨ë¸ íŠ¸ë ˆì´ë‹í•˜ê¸° (Training a language model from scratch with ğŸ¤— Transformers and TPUs)]({% link docs/06-examples/02-nlp/25-mlm_training_tpus.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[ì „ì´ í•™ìŠµìœ¼ë¡œ ê°ê´€ì‹ ê³¼ì œ (MultipleChoice Task with Transfer Learning)]({% link docs/06-examples/02-nlp/26-multiple_choice_task_with_transfer_learning.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Hugging Face íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ ì§ˆë¬¸ ë‹µë³€í•˜ê¸° (Question Answering with Hugging Face Transformers)]({% link docs/06-examples/02-nlp/27-question_answering.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Hugging Face íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•œ ì¶”ìƒì  ìš”ì•½ (Abstractive Summarization with Hugging Face Transformers)]({% link docs/06-examples/02-nlp/28-t5_hf_summarization.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}
