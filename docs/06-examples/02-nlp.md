---
layout: default
title: 자연어 처리
nav_order: 2
permalink: /examples/nlp/
parent: 코드 예제
has_children: true
---

* 원본 링크 : [https://keras.io/examples/nlp/](https://keras.io/examples/nlp/){:target="_blank"}
* 최종 수정일 : 2024-04-01

# 자연어 처리
{: .no_toc }

## 목차
{: .no_toc .text-delta }

1. TOC
{:toc}

---

### 텍스트 분류

★
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[Text classification from scratch]({% link docs/06-examples/02-nlp/01-text_classification_from_scratch.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Review Classification using Active Learning]({% link docs/06-examples/02-nlp/02-active_learning_review_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Text Classification using FNet]({% link docs/06-examples/02-nlp/03-fnet_classification_with_keras_nlp.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Large-scale multi-label text classification]({% link docs/06-examples/02-nlp/04-multi_label_classification.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Text classification with Transformer]({% link docs/06-examples/02-nlp/05-text_classification_with_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Text classification with Switch Transformer]({% link docs/06-examples/02-nlp/06-text_classification_with_switch_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Text classification using Decision Forests and pretrained embeddings]({% link docs/06-examples/02-nlp/07-tweet-classification-using-tfdf.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Using pre-trained word embeddings]({% link docs/06-examples/02-nlp/08-pretrained_word_embeddings.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Bidirectional LSTM on IMDB]({% link docs/06-examples/02-nlp/09-bidirectional_lstm_imdb.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Data Parallel Training with KerasNLP and tf.distribute]({% link docs/06-examples/02-nlp/10-data_parallel_training_with_keras_nlp.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 기계 번역

V3
{: .label .label-green .mx-1}
[English-to-Spanish translation with KerasNLP]({% link docs/06-examples/02-nlp/11-neural_machine_translation_with_keras_nlp.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

★
{: .label .label-purple .mx-1}
V3
{: .label .label-green .mx-1}
[English-to-Spanish translation with a sequence-to-sequence Transformer]({% link docs/06-examples/02-nlp/12-neural_machine_translation_with_transformer.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Character-level recurrent sequence-to-sequence model]({% link docs/06-examples/02-nlp/13-lstm_seq2seq.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 연관성 예측 (Entailment prediction)

V2
{: .label .label-yellow .mx-1}
[Multimodal entailment]({% link docs/06-examples/02-nlp/14-multimodal_entailment.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 명명된 엔티티 인식

V3
{: .label .label-green .mx-1}
[Named Entity Recognition using Transformers]({% link docs/06-examples/02-nlp/15-ner_transformers.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### Sequence-to-sequence

V2
{: .label .label-yellow .mx-1}
[Text Extraction with BERT]({% link docs/06-examples/02-nlp/16-text_extraction_with_bert.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Sequence to sequence learning for performing number addition]({% link docs/06-examples/02-nlp/17-addition_rnn.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 텍스트 유사도 검색

V3
{: .label .label-green .mx-1}
[Semantic Similarity with KerasNLP]({% link docs/06-examples/02-nlp/18-semantic_similarity_with_keras_nlp.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Semantic Similarity with BERT]({% link docs/06-examples/02-nlp/19-semantic_similarity_with_bert.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V3
{: .label .label-green .mx-1}
[Sentence embeddings using Siamese RoBERTa-networks]({% link docs/06-examples/02-nlp/20-sentence_embeddings_with_sbert.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 언어 모델링

V2
{: .label .label-yellow .mx-1}
[End-to-end Masked Language Modeling with BERT]({% link docs/06-examples/02-nlp/21-masked_language_modeling.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Pretraining BERT with Hugging Face Transformers]({% link docs/06-examples/02-nlp/22-pretraining_BERT.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 효율적인 매개변수 미세 조정

V3
{: .label .label-green .mx-1}
[Parameter-efficient fine-tuning of GPT-2 with LoRA]({% link docs/06-examples/02-nlp/23-parameter_efficient_finetuning_of_gpt2_with_lora.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

### 기타

V2
{: .label .label-yellow .mx-1}
[Abstractive Text Summarization with BART]({% link docs/06-examples/02-nlp/24-abstractive_summarization_with_bart.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Training a language model from scratch with 🤗 Transformers and TPUs]({% link docs/06-examples/02-nlp/25-mlm_training_tpus.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[MultipleChoice Task with Transfer Learning]({% link docs/06-examples/02-nlp/26-multiple_choice_task_with_transfer_learning.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Question Answering with Hugging Face Transformers]({% link docs/06-examples/02-nlp/27-question_answering.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}

V2
{: .label .label-yellow .mx-1}
[Abstractive Summarization with Hugging Face Transformers]({% link docs/06-examples/02-nlp/28-t5_hf_summarization.md %})
{: .d-inline .v-align-middle}

.
{: .lh-0 .my-0 .opacity-0}
