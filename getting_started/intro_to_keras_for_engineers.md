---
layout: default
title: ì—”ì§€ë‹ˆì–´ë¥¼ ìœ„í•œ Keras ì†Œê°œ
nav_order: 1
permalink: /getting_started/intro_to_keras_for_engineers
parent: ì‹œì‘í•˜ê¸°
---

* ì›ë³¸ ë§í¬ : [https://keras.io/getting_started/intro_to_keras_for_engineers/](https://keras.io/getting_started/intro_to_keras_for_engineers/)
* ìµœì¢… ìˆ˜ì •ì¼ : 2024-03-28

# ì—”ì§€ë‹ˆì–´ë¥¼ ìœ„í•œ Keras ì†Œê°œ
{: .no_toc }

## ëª©ì°¨
{: .no_toc .text-delta }

1. TOC
{:toc}

---

**ì €ì:** [fchollet](https://twitter.com/fchollet)  
**ìƒì„±ì¼:** 2023/07/10  
**ìµœì¢…í¸ì§‘ì¼:** 2023/07/10  
**ì„¤ëª…:** Keras 3ì™€ì˜ ì²« ì ‘ì´‰

[Colabì—ì„œ ë³´ê¸°](https://colab.research.google.com/github/keras-team/keras-io/blob/master/guides/ipynb/intro_to_keras_for_engineers.ipynb){: .btn .btn-blue }
[GitHub ì†ŒìŠ¤](https://github.com/keras-team/keras-io/blob/master/guides/intro_to_keras_for_engineers.py){: .btn .btn-blue }

----

## ì†Œê°œ

Keras 3ëŠ” TensorFlow, JAX ë° PyTorchì™€ ìƒí˜¸ í˜¸í™˜ë˜ëŠ” ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ì£¼ìš” Keras 3 ì›Œí¬í”Œë¡œìš°ë¥¼ ì•ˆë‚´í•©ë‹ˆë‹¤.

----

## ì…‹ì—…

ì—¬ê¸°ì„œëŠ” JAX ë°±ì—”ë“œë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ì•„ë˜ ë¬¸ìì—´ì„ ``"tensorflow"`` ë˜ëŠ” ``"torch"``ë¡œ ìˆ˜ì •í•˜ê³ , "Restart runtime"ì„ ëˆ„ë¥´ë©´, ë…¸íŠ¸ë¶ ì „ì²´ê°€ ë˜‘ê°™ì´ ì‹¤í–‰ë©ë‹ˆë‹¤! ì´ ì „ì²´ ê°€ì´ë“œëŠ” ë°±ì—”ë“œì— êµ¬ì• ë°›ì§€ ì•ŠìŠµë‹ˆë‹¤.

```python
import numpy as np
import os

os.environ["KERAS_BACKEND"] = "jax"

# KerasëŠ” ë°±ì—”ë“œê°€ êµ¬ì„±ëœ í›„ì— import ë˜ì–´ì•¼ í•œë‹¤ëŠ” ì ì— ìœ ì˜í•˜ì„¸ìš”. 
# íŒ¨í‚¤ì§€ë¥¼ import í•œ í›„ì—ëŠ”, ë°±ì—”ë“œë¥¼ ë³€ê²½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

import keras
```

----

## ì²« ë²ˆì§¸ ì˜ˆì‹œ: MNIST ì»¨ë¸Œë„·

MNIST ìˆ«ìë¥¼ ë¶„ë¥˜í•˜ê¸° ìœ„í•´ convnetì„ íŠ¸ë ˆì´ë‹í•˜ëŠ” MLì˜ Hello Worldë¶€í„° ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.

ë‹¤ìŒì€ ë°ì´í„°ì…ë‹ˆë‹¤:

```python
# ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  íŠ¸ë ˆì´ë‹ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• í•˜ê¸°
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# ì´ë¯¸ì§€ë¥¼ [0, 1] ë²”ìœ„ë¡œ ì¡°ì •í•˜ê¸°
x_train = x_train.astype("float32") / 255
x_test = x_test.astype("float32") / 255
# ì´ë¯¸ì§€ê°€ (28, 28, 1) ëª¨ì–‘ì¸ì§€ í™•ì¸í•˜ê¸°
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)
print("x_train shape:", x_train.shape)
print("y_train shape:", y_train.shape)
print(x_train.shape[0], "train samples")
print(x_test.shape[0], "test samples")
```

```
x_train shape: (60000, 28, 28, 1)
y_train shape: (60000,)
60000 train samples
10000 test samples
```

ì´ê²ƒì´ ìš°ë¦¬ì˜ ëª¨ë¸ì…ë‹ˆë‹¤.

Kerasì—ì„œ ì œê³µí•˜ëŠ” ë‹¤ì–‘í•œ ëª¨ë¸ ë¹Œë“œ ì˜µì…˜ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

*   [Sequential API](https://keras.io/guides/sequential_model/) (ì•„ë˜ì— ìš°ë¦¬ê°€ ì‚¬ìš©í•œ ê²ƒ)
*   [Functional API](https://keras.io/guides/functional_api/) (ê°€ì¥ ì¼ë°˜ì ì„)
*   [ì„œë¸Œí´ë˜ì‹±ì„ í†µí•´ ë‹¹ì‹ ë§Œì˜ ëª¨ë¸ ì‘ì„±í•˜ê¸°](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) (for advanced use cases)

```python
# ëª¨ë¸ íŒŒë¼ë¯¸í„°
num_classes = 10
input_shape = (28, 28, 1)

model = keras.Sequential(
    [
        keras.layers.Input(shape=input_shape),
        keras.layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
        keras.layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
        keras.layers.MaxPooling2D(pool_size=(2, 2)),
        keras.layers.Conv2D(128, kernel_size=(3, 3), activation="relu"),
        keras.layers.Conv2D(128, kernel_size=(3, 3), activation="relu"),
        keras.layers.GlobalAveragePooling2D(),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(num_classes, activation="softmax"),
    ]
)
```

ì´ê²ƒì´ ìš°ë¦¬ ëª¨ë¸ì˜ Summary ì…ë‹ˆë‹¤.:

```python
model.summary()
```

```
Model: "sequential"
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Layer (type)                    â”ƒ Output Shape              â”ƒ    Param # â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ conv2d (Conv2D)                 â”‚ (None, 26, 26, 64)        â”‚        640 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_1 (Conv2D)               â”‚ (None, 24, 24, 64)        â”‚     36,928 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ max_pooling2d (MaxPooling2D)    â”‚ (None, 12, 12, 64)        â”‚          0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_2 (Conv2D)               â”‚ (None, 10, 10, 128)       â”‚     73,856 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_3 (Conv2D)               â”‚ (None, 8, 8, 128)         â”‚    147,584 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ global_average_pooling2d        â”‚ (None, 128)               â”‚          0 â”‚
â”‚ (GlobalAveragePooling2D)        â”‚                           â”‚            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout (Dropout)               â”‚ (None, 128)               â”‚          0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense (Dense)                   â”‚ (None, 10)                â”‚      1,290 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 260,298 (1016.79 KB)
 Trainable params: 260,298 (1016.79 KB)
 Non-trainable params: 0 (0.00 B)
```

`compile()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜µí‹°ë§ˆì´ì €, ì†ì‹¤ í•¨ìˆ˜, ëª¨ë‹ˆí„°ë§í•  ë©”íŠ¸ë¦­ì„ ì§€ì •í•©ë‹ˆë‹¤. JAX ë° TensorFlow ë°±ì—”ë“œì—ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ XLA ì»´íŒŒì¼ì´ ì¼œì ¸ ìˆë‹¤ëŠ” ì ì— ìœ ì˜í•˜ì„¸ìš”.

```python
model.compile(
    loss=keras.losses.SparseCategoricalCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    metrics=[
        keras.metrics.SparseCategoricalAccuracy(name="acc"),
    ],
)
```

ëª¨ë¸ì„ íŠ¸ë ˆì´ë‹í•˜ê³  í‰ê°€í•´ ë³´ê² ìŠµë‹ˆë‹¤. ë³´ì§€ ì•Šì€ ë°ì´í„°ì— ëŒ€í•œ ì¼ë°˜í™”ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê¸° ìœ„í•´ íŠ¸ë ˆì´ë‹ ì¤‘ì— ë°ì´í„°ì˜ 15%ì— í•´ë‹¹í•˜ëŠ” ê²€ì¦ ë¶„í• ì„ ë”°ë¡œ ì„¤ì •í•˜ê² ìŠµë‹ˆë‹¤.

```python
batch_size = 128
epochs = 20

callbacks = [
    keras.callbacks.ModelCheckpoint(filepath="model_at_epoch_{epoch}.keras"),
    keras.callbacks.EarlyStopping(monitor="val_loss", patience=2),
]

model.fit(
    x_train,
    y_train,
    batch_size=batch_size,
    epochs=epochs,
    validation_split=0.15,
    callbacks=callbacks,
)
score = model.evaluate(x_test, y_test, verbose=0)
```

```
Epoch 1/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 74s 184ms/step - acc: 0.4980 - loss: 1.3832 - val_acc: 0.9609 - val_loss: 0.1513
Epoch 2/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 74s 186ms/step - acc: 0.9245 - loss: 0.2487 - val_acc: 0.9702 - val_loss: 0.0999
Epoch 3/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 70s 175ms/step - acc: 0.9515 - loss: 0.1647 - val_acc: 0.9816 - val_loss: 0.0608
Epoch 4/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 69s 174ms/step - acc: 0.9622 - loss: 0.1247 - val_acc: 0.9833 - val_loss: 0.0541
Epoch 5/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 68s 171ms/step - acc: 0.9685 - loss: 0.1083 - val_acc: 0.9860 - val_loss: 0.0468
Epoch 6/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 70s 176ms/step - acc: 0.9710 - loss: 0.0955 - val_acc: 0.9897 - val_loss: 0.0400
Epoch 7/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 69s 172ms/step - acc: 0.9742 - loss: 0.0853 - val_acc: 0.9888 - val_loss: 0.0388
Epoch 8/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 68s 169ms/step - acc: 0.9789 - loss: 0.0738 - val_acc: 0.9902 - val_loss: 0.0387
Epoch 9/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 75s 187ms/step - acc: 0.9789 - loss: 0.0691 - val_acc: 0.9907 - val_loss: 0.0341
Epoch 10/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 77s 194ms/step - acc: 0.9806 - loss: 0.0636 - val_acc: 0.9907 - val_loss: 0.0348
Epoch 11/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 74s 186ms/step - acc: 0.9812 - loss: 0.0610 - val_acc: 0.9926 - val_loss: 0.0271
Epoch 12/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 219s 550ms/step - acc: 0.9820 - loss: 0.0590 - val_acc: 0.9912 - val_loss: 0.0294
Epoch 13/20
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 70s 176ms/step - acc: 0.9843 - loss: 0.0504 - val_acc: 0.9918 - val_loss: 0.0316
```

íŠ¸ë ˆì´ë‹ ì¤‘ì—, ê° ì—í¬í¬ê°€ ëë‚  ë•Œë§ˆë‹¤ ëª¨ë¸ì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤. ì•„ë˜ì²˜ëŸ¼ ëª¨ë¸ì„ ìµœì‹  ìƒíƒœë¡œ ì €ì¥í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:

```python
model.save("final_model.keras")
```

ê·¸ë¦¬ê³  ì´ë ‡ê²Œ ë‹¤ì‹œ ë¡œë“œí•©ë‹ˆë‹¤:

```python
model = keras.saving.load_model("final_model.keras")
```

ë‹¤ìŒìœ¼ë¡œ, `predict()`ë¥¼ ì‚¬ìš©í•˜ì—¬ í´ë˜ìŠ¤ í™•ë¥  ì˜ˆì¸¡ì„ ì¿¼ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
predictions = model.predict(x_test)
```

```
 313/313 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 9ms/step
```

ê¸°ë³¸ ì‚¬í•­ì€ ì—¬ê¸°ê¹Œì§€ì…ë‹ˆë‹¤!

----

## í¬ë¡œìŠ¤ í”„ë ˆì„ì›Œí¬ ì‚¬ìš©ì ì •ì˜ ì»´í¬ë„ŒíŠ¸ ì‘ì„±

Keras enables you to write custom Layers, Models, Metrics, Losses, and Optimizers that work across TensorFlow, JAX, and PyTorch with the same codebase. Let's take a look at custom layers first.

The `keras.ops` namespace contains:

*   An implementation of the NumPy API, e.g. [`keras.ops.stack`](/api/ops/numpy#stack-function) or [`keras.ops.matmul`](/api/ops/numpy#matmul-function).
*   A set of neural network specific ops that are absent from NumPy, such as [`keras.ops.conv`](/api/ops/nn#conv-function) or [`keras.ops.binary_crossentropy`](/api/ops/nn#binarycrossentropy-function).

Let's make a custom `Dense` layer that works with all backends:

```python
class MyDense(keras.layers.Layer):
    def __init__(self, units, activation=None, name=None):
        super().__init__(name=name)
        self.units = units
        self.activation = keras.activations.get(activation)

    def build(self, input_shape):
        input_dim = input_shape[-1]
        self.w = self.add_weight(
            shape=(input_dim, self.units),
            initializer=keras.initializers.GlorotNormal(),
            name="kernel",
            trainable=True,
        )

        self.b = self.add_weight(
            shape=(self.units,),
            initializer=keras.initializers.Zeros(),
            name="bias",
            trainable=True,
        )

    def call(self, inputs):
        # Use Keras ops to create backend-agnostic layers/metrics/etc.
        x = keras.ops.matmul(inputs, self.w) + self.b
        return self.activation(x)
```

Next, let's make a custom `Dropout` layer that relies on the `keras.random` namespace:

```python
class MyDropout(keras.layers.Layer):
    def __init__(self, rate, name=None):
        super().__init__(name=name)
        self.rate = rate
        # Use seed_generator for managing RNG state.
        # It is a state element and its seed variable is
        # tracked as part of `layer.variables`.
        self.seed_generator = keras.random.SeedGenerator(1337)

    def call(self, inputs):
        # Use `keras.random` for random ops.
        return keras.random.dropout(inputs, self.rate, seed=self.seed_generator)
```

Next, let's write a custom subclassed model that uses our two custom layers:

```python
class MyModel(keras.Model):
    def __init__(self, num_classes):
        super().__init__()
        self.conv_base = keras.Sequential(
            [
                keras.layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
                keras.layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
                keras.layers.MaxPooling2D(pool_size=(2, 2)),
                keras.layers.Conv2D(128, kernel_size=(3, 3), activation="relu"),
                keras.layers.Conv2D(128, kernel_size=(3, 3), activation="relu"),
                keras.layers.GlobalAveragePooling2D(),
            ]
        )
        self.dp = MyDropout(0.5)
        self.dense = MyDense(num_classes, activation="softmax")

    def call(self, x):
        x = self.conv_base(x)
        x = self.dp(x)
        return self.dense(x)
```

Let's compile it and fit it:

```python
model = MyModel(num_classes=10)
model.compile(
    loss=keras.losses.SparseCategoricalCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    metrics=[
        keras.metrics.SparseCategoricalAccuracy(name="acc"),
    ],
)

model.fit(
    x_train,
    y_train,
    batch_size=batch_size,
    epochs=1,  # For speed
    validation_split=0.15,
)
```

```
 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 70s 174ms/step - acc: 0.5104 - loss: 1.3473 - val_acc: 0.9256 - val_loss: 0.2484

<keras.src.callbacks.history.History at 0x105608670>
```

----

## Training models on arbitrary data sources

All Keras models can be trained and evaluated on a wide variety of data sources, independently of the backend you're using. This includes:

*   NumPy arrays
*   Pandas dataframes
*   TensorFlow [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) objects
*   PyTorch `DataLoader` objects
*   Keras `PyDataset` objects

They all work whether you're using TensorFlow, JAX, or PyTorch as your Keras backend.

Let's try it out with PyTorch `DataLoaders`:

```python
import torch

# Create a TensorDataset
train_torch_dataset = torch.utils.data.TensorDataset(
    torch.from_numpy(x_train), torch.from_numpy(y_train)
)
val_torch_dataset = torch.utils.data.TensorDataset(
    torch.from_numpy(x_test), torch.from_numpy(y_test)
)

# Create a DataLoader
train_dataloader = torch.utils.data.DataLoader(
    train_torch_dataset, batch_size=batch_size, shuffle=True
)
val_dataloader = torch.utils.data.DataLoader(
    val_torch_dataset, batch_size=batch_size, shuffle=False
)

model = MyModel(num_classes=10)
model.compile(
    loss=keras.losses.SparseCategoricalCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    metrics=[
        keras.metrics.SparseCategoricalAccuracy(name="acc"),
    ],
)
model.fit(train_dataloader, epochs=1, validation_data=val_dataloader)
```

```
 469/469 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 81s 172ms/step - acc: 0.5502 - loss: 1.2550 - val_acc: 0.9419 - val_loss: 0.1972

<keras.src.callbacks.history.History at 0x2b3385480>
```

Now let's try this out with [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data):

```python
import tensorflow as tf

train_dataset = (
    tf.data.Dataset.from_tensor_slices((x_train, y_train))
    .batch(batch_size)
    .prefetch(tf.data.AUTOTUNE)
)
test_dataset = (
    tf.data.Dataset.from_tensor_slices((x_test, y_test))
    .batch(batch_size)
    .prefetch(tf.data.AUTOTUNE)
)

model = MyModel(num_classes=10)
model.compile(
    loss=keras.losses.SparseCategoricalCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    metrics=[
        keras.metrics.SparseCategoricalAccuracy(name="acc"),
    ],
)
model.fit(train_dataset, epochs=1, validation_data=test_dataset)
```

```
 469/469 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 81s 172ms/step - acc: 0.5771 - loss: 1.1948 - val_acc: 0.9229 - val_loss: 0.2502

<keras.src.callbacks.history.History at 0x2b33e7df0>
```

----

## Further reading

This concludes our short overview of the new multi-backend capabilities of Keras 3. Next, you can learn about:

### How to customize what happens in `fit()`

Want to implement a non-standard training algorithm yourself but still want to benefit from the power and usability of `fit()`? It's easy to customize `fit()` to support arbitrary use cases:

*   [Customizing what happens in `fit()` with TensorFlow](http://keras.io/guides/custom_train_step_in_tensorflow/)
*   [Customizing what happens in `fit()` with JAX](http://keras.io/guides/custom_train_step_in_jax/)
*   [Customizing what happens in `fit()` with PyTorch](http://keras.io/guides/custom_train_step_in_torch/)

* * *

How to write custom training loops
----------------------------------

*   [Writing a training loop from scratch in TensorFlow](http://keras.io/guides/writing_a_custom_training_loop_in_tensorflow/)
*   [Writing a training loop from scratch in JAX](http://keras.io/guides/writing_a_custom_training_loop_in_jax/)
*   [Writing a training loop from scratch in PyTorch](http://keras.io/guides/writing_a_custom_training_loop_in_torch/)

* * *

How to distribute training
--------------------------

*   [Guide to distributed training with TensorFlow](http://keras.io/guides/distributed_training_with_tensorflow/)
*   [JAX distributed training example](https://github.com/keras-team/keras/blob/master/examples/demo_jax_distributed.py)
*   [PyTorch distributed training example](https://github.com/keras-team/keras/blob/master/examples/demo_torch_multi_gpu.py)

Enjoy the library! ğŸš€